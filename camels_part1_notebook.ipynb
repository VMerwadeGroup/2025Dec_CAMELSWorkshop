{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86695295",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"><h1><center>Part 1: CAMELS-Like Data Preparation for USGS Basins – Catchment Attributes</center></h1></span>\n",
    "<center>Prepared by <br>\n",
    "<b>Mohammad Galib and Venkatesh Merwade</b><br> \n",
    "Lyles School of Civil Engineering, Purdue University<br>\n",
    "mgalib@purdue.edu, vmerwade@purdue.edu<br>\n",
    "<b><br>\n",
    "    FAIR Science in Water Resources</b><br></center>\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f9b84",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Table of Contents</span>\n",
    "\n",
    "1. [Introduction and Objectives](#1-introduction-and-objectives) \n",
    "2. [Understanding CAMELS: Background and Significance](#2-understanding-camels-background-and-significance)  \n",
    "3. [Tutorial Setup and Dependencies](#3-tutorial-setup-and-dependencies)  \n",
    "4. [Watershed Delineation](#4-watershed-delineation)  \n",
    "5. [Topographic Attributes](#5-topographic-attributes)  \n",
    "6. [Climatic Indices](#6-climatic-indices)  \n",
    "7. [Soil Characteristics](#7-soil-characteristics)  \n",
    "8. [Vegetation Characteristics](#8-vegetation-characteristics)  \n",
    "9. [Geological Characteristics](#9-geological-characteristics)  \n",
    "10. [Hydrological Signatures](#10-hydrological-signatures)  \n",
    "11. [Data Integration and Export](#11-data-integration-and-export)  \n",
    "12. [Results Validation](#12-results-validation)  \n",
    "13. [Extending to Multiple Basins](#13-extending-to-multiple-basins)  \n",
    "14. [Summary](#14-summary)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736751b2",
   "metadata": {},
   "source": [
    "<a id=\"1-introduction-and-objectives\"></a>\n",
    "## <span style=\"color:green\">1. Introduction and Objectives</span>\n",
    "\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "<p style='text-align: justify;'>\n",
    "By the end of this tutorial, you will be able to:\n",
    "</p>\n",
    "\n",
    "- Extract and process catchment-scale attributes for any USGS basin in the United States  \n",
    "- Understand the hydrological significance of each attribute class  \n",
    "- Create a reproducible workflow for large-sample hydrology studies  \n",
    "- Generate datasets compatible with machine learning and comparative hydrology applications  \n",
    "\n",
    "### <span style=\"color:green\">What You'll Build</span>\n",
    "<p style='text-align: justify;'>\n",
    "A comprehensive dataset containing:\n",
    "</p>\n",
    "\n",
    "- <b>Topographic attributes</b> (elevation, slope, area)  \n",
    "- <b>Climatic indices</b> (precipitation, temperature patterns, seasonality)  \n",
    "- <b>Soil properties</b> (depth, porosity, water storage capacity)  \n",
    "- <b>Vegetation characteristics</b> (land cover, LAI, root depth)  \n",
    "- <b>Geological features</b> (lithology, permeability)  \n",
    "- <b>Hydrological signatures</b> (flow statistics, event characteristics)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d3527",
   "metadata": {},
   "source": [
    "<a id=\"2-understanding-camels-background-and-significance\"></a>\n",
    "## <span style=\"color:green\">2. Understanding CAMELS: Background and Significance</span>\n",
    "\n",
    "### <span style=\"color:green\">What is CAMELS?</span>\n",
    "<p style='text-align: justify;'>\n",
    "CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) is a foundational dataset in hydrology that provides standardized attributes for 671 watersheds across the contiguous United States. Originally developed by Newman et al. (2015) and Addor et al. (2017), CAMELS has become the gold standard for large-sample hydrological studies.\n",
    "</p>\n",
    "\n",
    "### <span style=\"color:green\">Why CAMELS-like Datasets Matter</span>\n",
    "\n",
    "**For Hydrological Understanding**\n",
    "- Enable comparative analysis across different climatic and physiographic regions  \n",
    "- Support process-based understanding of catchment behavior  \n",
    "- Facilitate identification of dominant controls on hydrological response  \n",
    "\n",
    "**For Machine Learning Applications**\n",
    "- Provide standardized input features for predictive modeling  \n",
    "- Enable transfer learning between different catchments  \n",
    "- Support uncertainty quantification in hydrological predictions  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803c409",
   "metadata": {},
   "source": [
    "<a id=\"3-tutorial-setup-and-dependencies\"></a>\n",
    "## <span style=\"color:green\">3. Tutorial Setup and Dependencies</span>\n",
    "\n",
    "### <span style=\"color:green\">Required Libraries</span>\n",
    "<p style='text-align: justify;'>\n",
    "This tutorial relies on Python libraries for geospatial analysis, hydrological data access, and scientific computing. Before running the code cells, make sure you have installed all required dependencies in your environment.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from typing import Union\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Geospatial and hydrological libraries\n",
    "from pynhd import NLDI\n",
    "from streamstats import Watershed\n",
    "import py3dep\n",
    "import xrspatial\n",
    "import pygridmet as gridmet\n",
    "import hydrofunctions as hf\n",
    "#from pydaymet import get_bycoords\n",
    "import pygeohydro as gh\n",
    "from pygeohydro import NWIS, soil_properties, soil_polaris\n",
    "from pygeoutils import xarray_geomask\n",
    "from pygeoglim import glim_attributes, glhymps_attributes\n",
    "from shapely.geometry import box\n",
    "\n",
    "# Remote sensing and cloud platforms\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "import rioxarray\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "# Statistical and curve fitting\n",
    "from scipy.optimize import curve_fit\n",
    "from functools import reduce\n",
    "\n",
    "print(\"All required libraries loaded successfully!\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168f62d",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Environment Setup</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6597fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Create output directories\n",
    "import os\n",
    "os.makedirs('outputs/plots', exist_ok=True)\n",
    "os.makedirs('outputs/data', exist_ok=True)\n",
    "os.makedirs('outputs/shapefiles', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672c03d",
   "metadata": {},
   "source": [
    "<a id=\"4-watershed-delineation\"></a>\n",
    "## <span style=\"color:green\">4. Watershed Delineation</span>\n",
    "\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Understand watershed delineation principles  \n",
    "- Learn to use automated delineation tools  \n",
    "- Validate watershed boundaries  \n",
    "\n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align:justify;'>\n",
    "Watershed delineation is the foundation of catchment-scale analysis. The watershed boundary defines the area that contributes surface water flow to a specific outlet point (USGS gauge). Accurate delineation ensures that all computed attributes represent the actual contributing area. It leverages web services such as <b>USGS NLDI</b> to automatically retrieve standardized watershed geometries, ensuring reproducibility across studies.\n",
    "</p>\n",
    "\n",
    "### <span style=\"color:green\">Gauge Selection</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "    \"\"\"Interactive function to get USGS gauge ID from user\"\"\"\n",
    "    while True:\n",
    "        gauge_id = input(\"Enter USGS gauge ID (e.g., 01031500, 03291780): \").strip()\n",
    "        if len(gauge_id) >= 8 and gauge_id.isdigit():\n",
    "            return gauge_id\n",
    "        else:\n",
    "            print(\"Please enter a valid 8+ digit USGS gauge ID\")\n",
    "\n",
    "# Get gauge ID from user\n",
    "gauge_id = get_user_input()\n",
    "print(f\"Processing gauge: {gauge_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa70dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delineate_watershed(gauge_id, save_shapefile=True):\n",
    "    \"\"\"\n",
    "    Delineate watershed boundary for a given USGS gauge, \n",
    "    fetch metadata (name, lat, lon), and plot results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gauge_id : str\n",
    "        USGS gauge identifier\n",
    "    save_shapefile : bool\n",
    "        Whether to save the watershed boundary as shapefile\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (watershed_gdf, watershed_geom, metadata_dict)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Delineating watershed for gauge {gauge_id}...\")\n",
    "\n",
    "        # --- Delineate watershed ---\n",
    "        nldi = NLDI()\n",
    "        watershed_gdf = nldi.get_basins(gauge_id)\n",
    "\n",
    "        # Ensure CRS is WGS84\n",
    "        if watershed_gdf.crs != \"EPSG:4326\":\n",
    "            watershed_gdf = watershed_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "        watershed_geom = watershed_gdf.geometry.iloc[0]\n",
    "\n",
    "        # Calculate area\n",
    "        area_km2 = watershed_gdf.to_crs(\"EPSG:5070\").geometry.area.iloc[0] / 1e6\n",
    "\n",
    "        # Save shapefile if requested\n",
    "        if save_shapefile:\n",
    "            shapefile_path = f\"outputs/shapefiles/watershed_{gauge_id}.shp\"\n",
    "            watershed_gdf.to_file(shapefile_path)\n",
    "            print(f\"✓ Saved shapefile: {shapefile_path}\")\n",
    "\n",
    "        # --- Fetch gauge metadata ---\n",
    "        nwis = NWIS()\n",
    "        site_info = nwis.get_info([{\"site\": gauge_id}])\n",
    "        site_info[\"site_no\"] = site_info[\"site_no\"].astype(str)\n",
    "        row = site_info.loc[site_info[\"site_no\"] == str(gauge_id)].iloc[0]\n",
    "\n",
    "        gauge_lat = float(row[\"dec_lat_va\"])\n",
    "        gauge_lon = float(row[\"dec_long_va\"])\n",
    "        gauge_name = row[\"station_nm\"]\n",
    "\n",
    "        metadata = {\n",
    "            \"gauge_id\": str(gauge_id),\n",
    "            \"gauge_name\": gauge_name,\n",
    "            \"gauge_lat\": gauge_lat,\n",
    "            \"gauge_lon\": gauge_lon,\n",
    "            \"geometry\": watershed_geom\n",
    "        }\n",
    "\n",
    "        print(f\"✓ Successfully delineated watershed\")\n",
    "        print(f\"  Gauge Name: {gauge_name}\")\n",
    "        print(f\"  Area: {area_km2:.2f} km²\")\n",
    "        print(f\"  Location: ({gauge_lat:.3f}, {gauge_lon:.3f})\")\n",
    "\n",
    "        # --- Plot watershed with metadata ---\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        watershed_gdf.plot(ax=ax, facecolor=\"lightblue\", edgecolor=\"blue\", alpha=0.7)\n",
    "\n",
    "        ax.set_title(f\"Watershed for USGS Gauge {gauge_id}\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Longitude (°)\")\n",
    "        ax.set_ylabel(\"Latitude (°)\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Area annotation\n",
    "        ax.text(0.02, 0.98, f\"Area: {area_km2:.1f} km²\",\n",
    "                transform=ax.transAxes,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\"),\n",
    "                verticalalignment=\"top\")\n",
    "\n",
    "        # Metadata annotations\n",
    "        ax.text(0.02, 0.90, f\"Gauge: {gauge_name}\", transform=ax.transAxes, fontsize=10, ha=\"left\")\n",
    "        ax.text(0.02, 0.86, f\"Lat/Lon: {gauge_lat:.2f}, {gauge_lon:.2f}\",\n",
    "                transform=ax.transAxes, fontsize=10, ha=\"left\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"outputs/plots/watershed_{gauge_id}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "        return watershed_gdf, watershed_geom, metadata, area_km2\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error delineating watershed: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "# === Delineate ===\n",
    "watershed_gdf, watershed_geom, metadata, area_km2 = delineate_watershed(gauge_id)\n",
    "\n",
    "print(\"\\nMetadata:\")\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910016d9",
   "metadata": {},
   "source": [
    "<a id=\"5-topographic-attributes\"></a>\n",
    "## <span style=\"color:green\">5. Topographic Attributes</span>\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Understand how topography controls hydrological processes  \n",
    "- Extract elevation and slope statistics from DEM data  \n",
    "- Compute drainage area accurately  \n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align: justify;'>\n",
    "Topographic attributes are fundamental controls on hydrological processes. \n",
    "<b>Elevation</b> regulates temperature gradients, precipitation patterns, and snow accumulation/melt. \n",
    "<b>Slope</b> influences runoff velocity, infiltration capacity, and erosion potential. \n",
    "<b>Drainage area</b> defines the contributing surface area that generates flow toward the outlet. \n",
    "By combining these attributes, we can characterize the geomorphological setting of a watershed in a way that is both physically meaningful and consistent with CAMELS conventions.\n",
    "</p>\n",
    "\n",
    "**Attributes Extracted:**\n",
    "- **Elevation**: Mean (`elev_mean`), Minimum (`elev_min`), Maximum (`elev_max`), Standard Deviation (`elev_std`) — in meters (m)\n",
    "- **Slope**: Mean (`slope_mean`), Standard Deviation (`slope_std`) — in percent (%)\n",
    "- **Area**: Catchment Area (`area_geospa_fabric`) — in square kilometers (km²)\n",
    "\n",
    "### <span style=\"color:green\">Topographic Analysis Functions</span>\n",
    "<p style='text-align: justify;'>\n",
    "In this step, we use the <b>USGS 3DEP DEM</b> dataset to extract elevation and slope information for the watershed. \n",
    "We calculate mean, minimum, maximum, and variability of elevation and slope, along with the catchment drainage area. \n",
    "The results replicate the <b>camels_topo</b> attributes from CAMELS and provide essential descriptors for hydrological modeling and comparative basin analysis.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3628111-982e-4891-a285-ee4e0563534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topographic_attributes(watershed_geom, resolution=30):\n",
    "    \"\"\"\n",
    "    Extract topographic attributes from DEM data, aligned with CAMELS conventions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    watershed_geom : shapely.geometry\n",
    "        Watershed boundary geometry\n",
    "    resolution : int\n",
    "        DEM resolution in meters (default: 30m from 3DEP)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Topographic attributes (keys without units)\n",
    "    dict : Units for each attribute\n",
    "    xarray.DataArray : DEM\n",
    "    xarray.DataArray : Slope (m/km)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Extracting topographic attributes...\")\n",
    "        \n",
    "        # --- DEM ---\n",
    "        print(\"  - Downloading DEM data...\")\n",
    "        dem = py3dep.get_dem(watershed_geom, resolution=resolution)\n",
    "        dem_proj = dem.rio.reproject(\"EPSG:5070\")  # Equal-area projection\n",
    "        \n",
    "        # --- Slope ---\n",
    "        print(\"  - Computing slope...\")\n",
    "        slope_deg = xrspatial.slope(dem_proj)        # slope in degrees\n",
    "        slope_mpm = py3dep.deg2mpm(slope_deg)        # slope in m/m\n",
    "        slope_m_per_km = slope_mpm * 1000            # m/km (CAMELS convention) ✓\n",
    "        \n",
    "        print(\"  - Slope stats (m/km):\")\n",
    "        print(f\"    min={float(slope_m_per_km.min().values):.2f}, \"\n",
    "              f\"max={float(slope_m_per_km.max().values):.2f}, \"\n",
    "              f\"mean={float(slope_m_per_km.mean().values):.2f}\")\n",
    "        \n",
    "        # --- Elevation statistics ---\n",
    "        elevation_stats = {\n",
    "            \"elev_mean\": float(dem_proj.mean().values),\n",
    "            \"elev_min\": float(dem_proj.min().values),\n",
    "            \"elev_max\": float(dem_proj.max().values),\n",
    "            \"elev_std\": float(dem_proj.std().values),\n",
    "        }\n",
    "        \n",
    "        # --- Slope statistics ---\n",
    "        slope_stats = {\n",
    "            \"slope_mean\": float(slope_m_per_km.mean().values),\n",
    "            \"slope_std\": float(slope_m_per_km.std().values),\n",
    "        }\n",
    "        \n",
    "        # --- Drainage area (km²) ---\n",
    "        watershed_proj = gpd.GeoDataFrame([1], geometry=[watershed_geom], crs=\"EPSG:4326\").to_crs(\"EPSG:5070\")\n",
    "        area_km2 = watershed_proj.geometry.area.iloc[0] / 1e6\n",
    "        \n",
    "        topo_attrs = {\n",
    "            **elevation_stats,\n",
    "            **slope_stats,\n",
    "            \"area_geospa_fabric\": area_km2,\n",
    "        }\n",
    "        \n",
    "        # --- Units ---\n",
    "        topo_units = {\n",
    "            \"elev_mean\": \"m\",\n",
    "            \"elev_min\": \"m\",\n",
    "            \"elev_max\": \"m\",\n",
    "            \"elev_std\": \"m\",\n",
    "            \"slope_mean\": \"m/km\",\n",
    "            \"slope_std\": \"m/km\",\n",
    "            \"area_geospa_fabric\": \"km²\",\n",
    "        }\n",
    "        \n",
    "        print(\"✓ Topographic attributes extracted successfully\")\n",
    "        return topo_attrs, topo_units, dem_proj, slope_m_per_km\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error extracting topographic attributes: {str(e)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def plot_topographic_maps(dem, slope_m_per_km, watershed_geom, gauge_id):\n",
    "    \"\"\"Create topographic visualization maps with watershed boundary overlay\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Elevation map\n",
    "    dem.plot(ax=axes[0], cmap=\"terrain\", cbar_kwargs={\"label\": \"Elevation (m)\"})\n",
    "    axes[0].set_title(\"Digital Elevation Model\")\n",
    "    axes[0].set_xlabel(\"X Coordinate\")\n",
    "    axes[0].set_ylabel(\"Y Coordinate\")\n",
    "    \n",
    "    # Overlay watershed boundary\n",
    "    gpd.GeoDataFrame(geometry=[watershed_geom], crs=\"EPSG:4326\").to_crs(dem.rio.crs).boundary.plot(\n",
    "        ax=axes[0], edgecolor=\"black\", linewidth=1.5\n",
    "    )\n",
    "    \n",
    "    # Slope map (m/km)\n",
    "    slope_m_per_km.plot(ax=axes[1], cmap=\"YlOrRd\", cbar_kwargs={\"label\": \"Slope (m/km)\"})\n",
    "    axes[1].set_title(\"Slope (m/km)\")\n",
    "    axes[1].set_xlabel(\"X Coordinate\")\n",
    "    axes[1].set_ylabel(\"Y Coordinate\")\n",
    "    \n",
    "    # Overlay watershed boundary\n",
    "    gpd.GeoDataFrame(geometry=[watershed_geom], crs=\"EPSG:4326\").to_crs(dem.rio.crs).boundary.plot(\n",
    "        ax=axes[1], edgecolor=\"black\", linewidth=1.5\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"outputs/plots/topography_{gauge_id}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "topo_attrs, topo_units, dem_data, slope_data = extract_topographic_attributes(watershed_geom)\n",
    "\n",
    "if topo_attrs is not None:\n",
    "    print(\"\\nTopographic Attributes (with units):\")\n",
    "    for key, value in topo_attrs.items():\n",
    "        unit = topo_units.get(key, \"\")\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.3f} {unit}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value} {unit}\")\n",
    "    \n",
    "    # Plot DEM and slope (m/km)\n",
    "    plot_topographic_maps(dem_data, slope_data, watershed_geom, gauge_id)\n",
    "else:\n",
    "    print(\"Failed to extract topographic attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6becb3",
   "metadata": {},
   "source": [
    "<a id=\"6-climatic-indices\"></a>\n",
    "## <span style=\"color:green\">6. Climatic Indices</span>\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Understand climate controls on hydrological processes  \n",
    "- Compute seasonality and variability indices  \n",
    "- Extract extreme event statistics \n",
    "\n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align: justify;'>\n",
    "Climate is the primary driver of hydrological processes, controlling both the magnitude and variability of water fluxes in a catchment. \n",
    "<b>Mean precipitation and temperature</b> establish the fundamental water and energy budgets. \n",
    "<b>Seasonality</b> determines when water is available throughout the year. \n",
    "The <b>aridity index</b> provides insight into water stress conditions by comparing potential evapotranspiration and precipitation. \n",
    "<b>Snow fraction</b> influences runoff timing in cold regions, while <b>extreme events</b> (heavy rainfall or prolonged droughts) shape hydrological extremes such as floods and low flows. \n",
    "Together, these indices replicate the <b>camels_clim</b> attributes used in large-sample hydrology.\n",
    "</p>\n",
    "\n",
    "**Attributes Extracted:**\n",
    "- **Mean Precipitation (`p_mean`)**: Long-term average daily precipitation (mm/day)\n",
    "- **Mean PET (`pet_mean`)**: Long-term average daily potential evapotranspiration (mm/day)\n",
    "- **Mean Temperature (`temp_mean`)**: Long-term average daily temperature (°C)\n",
    "- **Aridity Index (`aridity`)**: Ratio of PET to precipitation (dimensionless)\n",
    "- **Precipitation Seasonality (`p_seasonality`)**: Seasonal variation in precipitation (dimensionless)\n",
    "- **Temperature Seasonality (`temp_seasonality`)**: Seasonal variation in temperature (°C)\n",
    "- **Snow Fraction (`frac_snow`)**: Fraction of precipitation falling as snow\n",
    "- **High Precipitation**: Frequency (`high_prec_freq`, days/yr), Duration (`high_prec_dur`, days), Timing (`high_prec_timing`, DOY)\n",
    "- **Low Precipitation**: Frequency (`low_prec_freq`, days/yr), Duration (`low_prec_dur`, days), Timing (`low_prec_timing`, DOY)\n",
    "- **Precipitation Intensity (`prec_intensity`)**: Mean precipitation on wet days (mm/day)\n",
    "\n",
    "### <span style=\"color:green\">Climate Analysis Functions</span>\n",
    "<p style='text-align: justify;'>\n",
    "In this step, we use <b>GridMET daily climate data</b> to compute basin-averaged precipitation, temperature, and potential evapotranspiration. \n",
    "We then derive CAMELS-style indices, including precipitation and temperature seasonality, aridity, snow fraction, and extreme precipitation statistics. \n",
    "These indices serve as standardized features for hydrological modeling and machine learning applications.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_climate_data(watershed_geom, start_date=\"2000-01-01\", end_date=\"2020-12-31\"):\n",
    "    \"\"\"\n",
    "    Fetch climate data for the watershed\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    watershed_geom : shapely.geometry\n",
    "        Watershed boundary\n",
    "    start_date, end_date : str\n",
    "        Date range for data extraction\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.Dataset : Climate data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Fetching climate data from GridMET...\")\n",
    "        \n",
    "        variables = [\"tmmn\", \"tmmx\", \"pr\", \"pet\"]\n",
    "        \n",
    "        # Get daily GridMET data\n",
    "        ds = gridmet.get_bygeom(\n",
    "            geometry=watershed_geom,\n",
    "            dates=(start_date, end_date),\n",
    "            variables=variables,\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        \n",
    "        # Unit conversions\n",
    "        ds['tmmn'] = ds['tmmn'] - 273.15  # K to °C\n",
    "        ds['tmmx'] = ds['tmmx'] - 273.15  # K to °C\n",
    "        ds['tavg'] = (ds['tmmn'] + ds['tmmx']) / 2  # Average temperature\n",
    "        \n",
    "        print(f\"✓ Retrieved {len(ds.time)} days of climate data\")\n",
    "        return ds\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error fetching climate data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d58ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_climate_indices(climate_ds):\n",
    "    \"\"\"\n",
    "    Compute CAMELS-style climate indices (values only).\n",
    "    Units are provided separately in `climate_units`.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Computing climate indices...\")\n",
    "        \n",
    "        # Basin-averaged time series\n",
    "        tavg = climate_ds['tavg'].mean(dim=['lat', 'lon'])\n",
    "        prcp = climate_ds['pr'].mean(dim=['lat', 'lon'])\n",
    "        pet = climate_ds['pet'].mean(dim=['lat', 'lon'])\n",
    "        \n",
    "        # Basic statistics\n",
    "        p_mean = float(prcp.mean())\n",
    "        pet_mean = float(pet.mean())\n",
    "        temp_mean = float(tavg.mean())\n",
    "        \n",
    "        # Aridity index\n",
    "        aridity = pet_mean / p_mean if p_mean > 0 else np.inf\n",
    "        \n",
    "        # Seasonality analysis\n",
    "        seasonality_indices = compute_seasonality(tavg.values, prcp.values, climate_ds.time.values)\n",
    "        \n",
    "        # Snow fraction\n",
    "        frac_snow = compute_snow_fraction(tavg.values, prcp.values)\n",
    "        \n",
    "        # Extreme precipitation events (with duration + timing)\n",
    "        extreme_precip = compute_extreme_precipitation_stats(prcp.values, climate_ds.time.values)\n",
    "        \n",
    "        # Combine all indices (values only)\n",
    "        climate_indices = {\n",
    "            \"p_mean\": p_mean,\n",
    "            \"pet_mean\": pet_mean,\n",
    "            \"temp_mean\": temp_mean,\n",
    "            \"aridity\": aridity,\n",
    "            \"p_seasonality\": seasonality_indices[\"p_seasonality\"],\n",
    "            \"temp_seasonality\": seasonality_indices[\"temp_seasonality\"],\n",
    "            \"frac_snow\": frac_snow,\n",
    "            \"high_prec_freq\": extreme_precip[\"high_prec_freq\"],\n",
    "            \"high_prec_dur\": extreme_precip[\"high_prec_dur\"],\n",
    "            \"high_prec_timing\": extreme_precip[\"high_prec_timing\"],\n",
    "            \"low_prec_freq\": extreme_precip[\"low_prec_freq\"],\n",
    "            \"low_prec_dur\": extreme_precip[\"low_prec_dur\"],\n",
    "            \"low_prec_timing\": extreme_precip[\"low_prec_timing\"],\n",
    "            \"prec_intensity\": extreme_precip[\"prec_intensity\"],\n",
    "        }\n",
    "        \n",
    "        # Units dictionary (parallel to climate_indices)\n",
    "        climate_units = {\n",
    "            \"p_mean\": \"mm/day\",\n",
    "            \"pet_mean\": \"mm/day\",\n",
    "            \"temp_mean\": \"°C\",\n",
    "            \"aridity\": \"–\",\n",
    "            \"p_seasonality\": \"–\",\n",
    "            \"temp_seasonality\": \"°C\",\n",
    "            \"frac_snow\": \"fraction\",\n",
    "            \"high_prec_freq\": \"days/year\",\n",
    "            \"high_prec_dur\": \"days\",\n",
    "            \"high_prec_timing\": \"DOY\",\n",
    "            \"low_prec_freq\": \"days/year\",\n",
    "            \"low_prec_dur\": \"days\",\n",
    "            \"low_prec_timing\": \"DOY\",\n",
    "            \"prec_intensity\": \"mm/day\",\n",
    "        }\n",
    "        \n",
    "        print(\"✓ Climate indices computed successfully\")\n",
    "        return climate_indices, climate_units\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error computing climate indices: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "def compute_seasonality(temp, prcp, dates):\n",
    "    \"\"\"Compute seasonality indices using sinusoidal regression\"\"\"\n",
    "    try:\n",
    "        dates_dt = pd.to_datetime(dates)\n",
    "        doy = dates_dt.dayofyear.values\n",
    "        \n",
    "        # Temperature seasonality\n",
    "        def sine_temp(day_of_year, delta_t, s_t):\n",
    "            return delta_t * np.sin(2 * np.pi * (day_of_year - s_t) / 365.25)\n",
    "        \n",
    "        temp_detrended = temp - np.mean(temp)\n",
    "        temp_params, _ = curve_fit(sine_temp, doy, temp_detrended, p0=[10, -90])\n",
    "        \n",
    "        # Precipitation seasonality\n",
    "        def sine_prec(day_of_year, delta_p, s_p):\n",
    "            return 1 + delta_p * np.sin(2 * np.pi * (day_of_year - s_p) / 365.25)\n",
    "        \n",
    "        prcp_normalized = prcp / np.mean(prcp) - 1\n",
    "        prcp_params, _ = curve_fit(sine_prec, doy, prcp_normalized, p0=[0.4, 90])\n",
    "        \n",
    "        # Compute phase relationship\n",
    "        delta_t, s_t = temp_params\n",
    "        delta_p, s_p = prcp_params\n",
    "        \n",
    "        p_seasonality = delta_p * np.sign(delta_t) * np.cos(2 * np.pi * (s_p - s_t) / 365.25)\n",
    "        \n",
    "        return {\n",
    "            'p_seasonality': p_seasonality,\n",
    "            'temp_seasonality': delta_t\n",
    "        }\n",
    "    except:\n",
    "        return {'p_seasonality': np.nan, 'temp_seasonality': np.nan}\n",
    "\n",
    "def compute_snow_fraction(temp, prcp):\n",
    "    \"\"\"Compute fraction of precipitation falling as snow\"\"\"\n",
    "    try:\n",
    "        snow_days = (temp <= 0) & (prcp > 0)\n",
    "        if np.sum(prcp > 0) == 0:\n",
    "            return 0.0\n",
    "        return np.sum(prcp[snow_days]) / np.sum(prcp[prcp > 0])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def compute_extreme_precipitation_stats(prcp, dates=None):\n",
    "    \"\"\"\n",
    "    Compute extreme precipitation event statistics (CAMELS-style).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prcp : array-like\n",
    "        Daily precipitation [mm/day].\n",
    "    dates : array-like of datetime64, optional\n",
    "        Dates corresponding to precipitation series (needed for timing indices).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Extreme precipitation metrics.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prcp = np.asarray(prcp)\n",
    "        mean_prcp = np.mean(prcp)\n",
    "        n_days = len(prcp)\n",
    "\n",
    "        # --- High precipitation events (>5x mean) ---\n",
    "        high_mask = prcp >= 5 * mean_prcp\n",
    "        high_prec_freq = np.sum(high_mask) / n_days * 365.25\n",
    "        \n",
    "        # Duration (mean consecutive run length)\n",
    "        high_dur = []\n",
    "        count = 0\n",
    "        for v in high_mask:\n",
    "            if v:\n",
    "                count += 1\n",
    "            elif count > 0:\n",
    "                high_dur.append(count)\n",
    "                count = 0\n",
    "        if count > 0:\n",
    "            high_dur.append(count)\n",
    "        high_prec_dur = np.mean(high_dur) if high_dur else np.nan\n",
    "        \n",
    "        # Timing (center of mass of event days in DOY)\n",
    "        if dates is not None and np.any(high_mask):\n",
    "            doy = pd.to_datetime(dates).dayofyear.values\n",
    "            high_prec_timing = np.average(doy[high_mask], weights=prcp[high_mask])\n",
    "        else:\n",
    "            high_prec_timing = np.nan\n",
    "\n",
    "        # --- Low precipitation events (<1 mm/day) ---\n",
    "        low_mask = prcp < 1.0\n",
    "        low_prec_freq = np.sum(low_mask) / n_days * 365.25\n",
    "        \n",
    "        # Duration\n",
    "        low_dur = []\n",
    "        count = 0\n",
    "        for v in low_mask:\n",
    "            if v:\n",
    "                count += 1\n",
    "            elif count > 0:\n",
    "                low_dur.append(count)\n",
    "                count = 0\n",
    "        if count > 0:\n",
    "            low_dur.append(count)\n",
    "        low_prec_dur = np.mean(low_dur) if low_dur else np.nan\n",
    "        \n",
    "        # Timing\n",
    "        if dates is not None and np.any(low_mask):\n",
    "            doy = pd.to_datetime(dates).dayofyear.values\n",
    "            low_prec_timing = np.average(doy[low_mask], weights=np.ones_like(prcp[low_mask]))\n",
    "        else:\n",
    "            low_prec_timing = np.nan\n",
    "        \n",
    "        return {\n",
    "            'high_prec_freq': high_prec_freq,\n",
    "            'high_prec_dur': high_prec_dur,\n",
    "            'high_prec_timing': high_prec_timing,\n",
    "            'low_prec_freq': low_prec_freq,\n",
    "            'low_prec_dur': low_prec_dur,\n",
    "            'low_prec_timing': low_prec_timing,\n",
    "            'prec_intensity': np.mean(prcp[prcp > 0]) if np.any(prcp > 0) else np.nan\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"    Error in extreme precip stats: {e}\")\n",
    "        return {\n",
    "            'high_prec_freq': np.nan, 'high_prec_dur': np.nan, 'high_prec_timing': np.nan,\n",
    "            'low_prec_freq': np.nan, 'low_prec_dur': np.nan, 'low_prec_timing': np.nan,\n",
    "            'prec_intensity': np.nan\n",
    "        }\n",
    "\n",
    "\n",
    "# Fetch basin-averaged climate data\n",
    "climate_ds = fetch_climate_data(watershed_geom)\n",
    "\n",
    "if climate_ds is not None:\n",
    "    # Compute indices (values + units)\n",
    "    climate_attrs, climate_units = compute_climate_indices(climate_ds)\n",
    "    \n",
    "    if climate_attrs is not None:\n",
    "        print(\"\\nClimate Indices:\")\n",
    "        for key, value in climate_attrs.items():\n",
    "            unit = climate_units.get(key, \"\")\n",
    "            if isinstance(value, float):\n",
    "                if np.isnan(value):\n",
    "                    print(f\"  {key}: N/A {unit}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {value:.3f} {unit}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value} {unit}\")\n",
    "    else:\n",
    "        print(\"✗ Failed to compute climate attributes\")\n",
    "        climate_attrs, climate_units = {}, {}\n",
    "else:\n",
    "    print(\"✗ Failed to extract climate data\")\n",
    "    climate_attrs, climate_units = {}, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8b42b",
   "metadata": {},
   "source": [
    "<a id=\"7-soil-characteristics\"></a>\n",
    "\n",
    "## <span style=\"color:green\">7. Soil Characteristics</span>\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Understand how soil properties influence infiltration and water storage  \n",
    "- Access national soil datasets using Python (POLARIS, gNATSGO)  \n",
    "- Compute soil water storage capacity using porosity and thickness \n",
    "\n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align: justify;'>\n",
    "Soils are a critical control on catchment hydrology, influencing infiltration rates, subsurface storage, and the timing of runoff generation. \n",
    "<b>Soil porosity</b> determines the maximum volume of water that can be held within the soil matrix. \n",
    "<b>Soil thickness</b> defines the vertical profile available for storage, and together with porosity, controls the <b>maximum water content</b> a catchment can retain. \n",
    "The <b>storage capacity</b> is computed as:\n",
    "</p>\n",
    "\n",
    "$$\n",
    "\\text{Storage Capacity} = \\text{Porosity} \\times \\text{Thickness}\n",
    "$$\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This value represents the upper limit of water that soils can store before saturation and runoff occur. \n",
    "Additional soil properties such as <b>texture fractions</b> (sand, silt, clay) regulate hydraulic conductivity, while datasets like <b>POLARIS</b> and <b>gNATSGO</b> provide high-resolution, nationally consistent soil data accessible via the <b>Planetary Computer STAC API</b>. \n",
    "These attributes replicate the <b>camels_soil</b> component of the CAMELS dataset.\n",
    "</p>\n",
    "\n",
    "**Attributes Extracted:**\n",
    "- **Porosity (`soil_porosity`)**: Fraction of soil volume that is pore space (dimensionless)\n",
    "- **Available Water Capacity (`available_water_capacity`)**: Water available to plants (dimensionless)\n",
    "- **Field Capacity (`field_capacity`)**: Water content after drainage (dimensionless)\n",
    "- **Texture**: Fractions of Sand (`sand_frac`), Silt (`silt_frac`), Clay (`clay_frac`) — in percent (%)\n",
    "- **Soil Depth (`soil_depth_statsgo`)**: Depth to bedrock — in meters (m)\n",
    "- **Storage Capacity (`max_water_content`)**: Maximum water storage in soil profile — in meters (m)\n",
    "- **Conductivity (`soil_conductivity`)**: Saturated hydraulic conductivity — in log10(mm/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_soil_attributes(geom, crs=\"EPSG:4326\"):\n",
    "    \"\"\"\n",
    "    Extract soil attributes from multiple sources:\n",
    "      - Porosity, AWC, FC from USGS ScienceBase datasets (pygeohydro.soil_properties)\n",
    "      - Texture fractions + Ksat from POLARIS\n",
    "      - Soil thickness from gNATSGO rasters (Planetary Computer)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Extracting soil attributes...\")\n",
    "\n",
    "        # Step 1: Porosity, AWC, FC (from soil_properties)\n",
    "        ds_por = soil_properties(\"por\")\n",
    "        ds_por = xarray_geomask(ds_por, geom, crs)\n",
    "        ds_por = ds_por.where(ds_por.porosity > ds_por.porosity.rio.nodata)\n",
    "        ds_por[\"porosity\"] = ds_por.porosity.rio.write_nodata(np.nan) / 1000.0\n",
    "        print(\"  ✓ Porosity extracted\")\n",
    "\n",
    "        ds_awc = soil_properties(\"awc\")\n",
    "        ds_awc = xarray_geomask(ds_awc, geom, crs)\n",
    "        ds_awc = ds_awc.where(ds_awc.awc > ds_awc.awc.rio.nodata)\n",
    "        ds_awc[\"available_water_capacity\"] = ds_awc.awc.rio.write_nodata(np.nan) / 1000.0\n",
    "        print(\"  ✓ AWC extracted\")\n",
    "\n",
    "        ds_fc = soil_properties(\"fc\")\n",
    "        ds_fc = xarray_geomask(ds_fc, geom, crs)\n",
    "        ds_fc = ds_fc.where(ds_fc.fc > ds_fc.fc.rio.nodata)\n",
    "        ds_fc[\"field_capacity\"] = ds_fc.fc.rio.write_nodata(np.nan) / 1000.0\n",
    "        print(\"  ✓ Field Capacity extracted\")\n",
    "\n",
    "        # Step 2: Texture + Hydraulic conductivity (from POLARIS)\n",
    "        print(\"  - Downloading POLARIS texture + ksat...\")\n",
    "        requested = [\n",
    "            \"sand_5\", \"sand_15\", \"sand_30\",\n",
    "            \"silt_5\", \"silt_15\", \"silt_30\",\n",
    "            \"clay_5\", \"clay_15\", \"clay_30\",\n",
    "            \"ksat_5\", \"ksat_15\", \"ksat_30\"\n",
    "        ]\n",
    "        ds_tex = soil_polaris(requested, geom, geo_crs=crs)\n",
    "\n",
    "        def pick_var(var_base, depth_key):\n",
    "            name_try = f\"{var_base}_{depth_key}\"\n",
    "            if name_try in ds_tex.data_vars:\n",
    "                return ds_tex[name_try]\n",
    "            for v in ds_tex.data_vars:\n",
    "                if v.startswith(var_base) and depth_key in v:\n",
    "                    return ds_tex[v]\n",
    "            return None\n",
    "\n",
    "        sand_layers, silt_layers, clay_layers, ksat_layers = [], [], [], []\n",
    "        for d in [\"5\", \"15\", \"30\"]:\n",
    "            if (sv := pick_var(\"sand\", d)) is not None: \n",
    "                sand_layers.append(sv)\n",
    "            if (tv := pick_var(\"silt\", d)) is not None: \n",
    "                silt_layers.append(tv)\n",
    "            if (cv := pick_var(\"clay\", d)) is not None: \n",
    "                clay_layers.append(cv)\n",
    "            if (kv := pick_var(\"ksat\", d)) is not None: \n",
    "                ksat_layers.append(kv)\n",
    "\n",
    "        if not (sand_layers and silt_layers and clay_layers):\n",
    "            raise RuntimeError(\"Could not find matching texture layers in POLARIS\")\n",
    "\n",
    "        sand = sum(sand_layers) / len(sand_layers) / 100.0\n",
    "        silt = sum(silt_layers) / len(silt_layers) / 100.0\n",
    "        clay = sum(clay_layers) / len(clay_layers) / 100.0\n",
    "        print(f\"    ✓ Texture fractions extracted ({len(sand_layers)} depth layers)\")\n",
    "\n",
    "        # Soil conductivity\n",
    "        soil_conductivity_log10 = None\n",
    "        if ksat_layers:\n",
    "            soil_conductivity_mps = (sum(ksat_layers) / len(ksat_layers)) * 1e-6\n",
    "            soil_conductivity_mmhr = soil_conductivity_mps * 3.6e6\n",
    "            soil_conductivity_log10 = np.log10(soil_conductivity_mmhr.mean(skipna=True).item())\n",
    "            print(\"    ✓ Soil conductivity extracted\")\n",
    "        else:\n",
    "            print(\"    ⚠ No ksat layers found in POLARIS\")\n",
    "\n",
    "        # Step 3: Thickness from gNATSGO\n",
    "        print(\"  - Fetching gNATSGO thickness data...\")\n",
    "        client = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "        # Search using WGS84 bounds\n",
    "        search = client.search(\n",
    "            collections=[\"gnatsgo-rasters\"], \n",
    "            bbox=geom.bounds,\n",
    "            limit=10\n",
    "        )\n",
    "        items = list(search.get_items())\n",
    "\n",
    "        if not items:\n",
    "            raise ValueError(\"No gNATSGO items found for this basin\")\n",
    "\n",
    "        # Reproject geometry to gNATSGO CRS for clipping\n",
    "        gnatsgo_crs = \"ESRI:102039\"\n",
    "        geom_reprojected = gpd.GeoSeries([geom], crs=crs).to_crs(gnatsgo_crs).iloc[0]\n",
    "\n",
    "        # Find tile that intersects with geometry\n",
    "        thickness = None\n",
    "        for item in items:\n",
    "            signed_item = planetary_computer.sign(item)\n",
    "            temp_thickness = rioxarray.open_rasterio(\n",
    "                signed_item.assets[\"tk0_999a\"].href, \n",
    "                masked=True\n",
    "            )\n",
    "            \n",
    "            tile_bounds = temp_thickness.rio.bounds()\n",
    "            if geom_reprojected.intersects(box(*tile_bounds)):\n",
    "                thickness = temp_thickness\n",
    "                break\n",
    "            temp_thickness.close()\n",
    "\n",
    "        if thickness is None:\n",
    "            raise ValueError(\"No gNATSGO tile covers this watershed\")\n",
    "\n",
    "        # Clip and process thickness\n",
    "        thickness_clipped = thickness.rio.clip(\n",
    "            [geom_reprojected], \n",
    "            crs=gnatsgo_crs, \n",
    "            all_touched=True\n",
    "        )\n",
    "        thickness_clipped = thickness_clipped.where(thickness_clipped < 2e6) * 10  # cm → mm\n",
    "        thickness_clipped = thickness_clipped.rio.write_nodata(np.nan)\n",
    "        print(\"  ✓ Thickness extracted\")\n",
    "\n",
    "        # Resample and compute storage\n",
    "        thickness_resampled = thickness_clipped.rio.reproject_match(\n",
    "            ds_por[\"porosity\"], \n",
    "            resampling=5\n",
    "        )\n",
    "        storage = ds_por[\"porosity\"] * thickness_resampled\n",
    "        print(\"  ✓ Storage capacity computed\")\n",
    "\n",
    "        # === Final scalars ===\n",
    "        soil_attrs = {\n",
    "            \"soil_porosity\": float(ds_por[\"porosity\"].mean(skipna=True).item()),\n",
    "            \"available_water_capacity\": float(ds_awc[\"available_water_capacity\"].mean(skipna=True).item()),\n",
    "            \"field_capacity\": float(ds_fc[\"field_capacity\"].mean(skipna=True).item()),\n",
    "            \"sand_frac\": float(sand.mean(skipna=True).item()) * 100.0,\n",
    "            \"silt_frac\": float(silt.mean(skipna=True).item()) * 100.0,\n",
    "            \"clay_frac\": float(clay.mean(skipna=True).item()) * 100.0,\n",
    "            \"soil_depth_statsgo\": float(thickness_resampled.mean(skipna=True).item()) * 1e-3,  # mm → m\n",
    "            \"max_water_content\": float(storage.mean(skipna=True).item()) * 1e-3,  # mm → m\n",
    "        }\n",
    "        \n",
    "        if soil_conductivity_log10 is not None:\n",
    "            soil_attrs[\"soil_conductivity\"] = soil_conductivity_log10\n",
    "\n",
    "        soil_units = {\n",
    "            \"soil_porosity\": \"–\",\n",
    "            \"available_water_capacity\": \"–\",\n",
    "            \"field_capacity\": \"–\",\n",
    "            \"sand_frac\": \"%\",\n",
    "            \"silt_frac\": \"%\",\n",
    "            \"clay_frac\": \"%\",\n",
    "            \"soil_depth_statsgo\": \"m\",\n",
    "            \"max_water_content\": \"m\",\n",
    "            \"soil_conductivity\": \"log10(mm/h)\" if soil_conductivity_log10 is not None else \"–\"\n",
    "        }\n",
    "\n",
    "        print(\"✓ Soil attributes extracted successfully\")\n",
    "        return soil_attrs, soil_units\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error extracting soil attributes: {str(e)}\")\n",
    "        print(\"Using fallback values...\")\n",
    "        \n",
    "        soil_attrs = {\n",
    "            \"soil_porosity\": 0.42,\n",
    "            \"available_water_capacity\": 0.18,\n",
    "            \"field_capacity\": 0.29,\n",
    "            \"sand_frac\": 41.0,\n",
    "            \"silt_frac\": 36.0,\n",
    "            \"clay_frac\": 23.0,\n",
    "            \"soil_depth_statsgo\": 1.030,\n",
    "            \"max_water_content\": 0.520,\n",
    "            \"soil_conductivity\": 1e-6\n",
    "        }\n",
    "        \n",
    "        soil_units = {\n",
    "            k: (\"m\" if \"depth\" in k or \"content\" in k else \"–\") \n",
    "            for k in soil_attrs.keys()\n",
    "        }\n",
    "        soil_units[\"soil_conductivity\"] = \"log10(mm/h)\"\n",
    "        \n",
    "        return soil_attrs, soil_units\n",
    "\n",
    "\n",
    "# === Extract Soil Attributes ===\n",
    "soil_attrs, soil_units = extract_soil_attributes(watershed_geom)\n",
    "\n",
    "print(\"\\nSoil Attributes:\")\n",
    "for key, value in soil_attrs.items():\n",
    "    unit = soil_units.get(key, \"\")\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.3f} {unit}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value} {unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac7c8e",
   "metadata": {},
   "source": [
    "<a id=\"8-vegetation-characteristics\"></a>\n",
    "## <span style=\"color:green\">8. Vegetation Characteristics</span>\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Understand vegetation controls on evapotranspiration and water partitioning  \n",
    "- Extract land cover and vegetation indices from satellite data (MODIS & NLCD)  \n",
    "- Compute vegetation seasonal dynamics using multi-year climatology  \n",
    "- Apply CAMELS methodology for large-sample hydrology studies  \n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align: justify;'>\n",
    "Vegetation strongly regulates the land–atmosphere exchange of water and energy. \n",
    "<b>Leaf Area Index (LAI)</b> governs canopy transpiration and rainfall interception, while <b>land cover type</b> defines surface roughness and evapotranspiration rates. \n",
    "<b>Root depth</b> determines plant access to soil moisture and thus resilience to drought. \n",
    "Finally, <b>seasonal dynamics</b> captured by indices such as NDVI/GVF reflect vegetation activity across the growing cycle, directly influencing hydrological partitioning.\n",
    "</p>\n",
    "\n",
    "**Attributes Extracted:**\n",
    "- **LAI (Leaf Area Index)**:\n",
    "    - `lai_max`: Maximum monthly climatology (peak growing season)\n",
    "    - `lai_min`: Minimum monthly climatology (dormant season)\n",
    "    - `lai_diff`: Seasonal amplitude (max - min)\n",
    "- **GVF (Green Vegetation Fraction)**:\n",
    "    - `gvf_max`: Maximum monthly green vegetation fraction\n",
    "    - `gvf_mean`: Annual mean green vegetation fraction\n",
    "    - `gvf_diff`: Seasonal amplitude of greenness\n",
    "- **Land Cover Fractions**:\n",
    "    - `frac_forest`: Fraction of basin covered by forest\n",
    "    - `frac_cropland`: Fraction of basin covered by cropland\n",
    "    - `water_frac`: Fraction of basin covered by open water\n",
    "- **Dominant Land Cover**:\n",
    "    - `dom_land_cover`: Most frequent land cover class\n",
    "    - `dom_land_cover_frac`: Fraction of the basin covered by the dominant class\n",
    "- **Root Depth**:\n",
    "    - `root_depth_50`: Soil depth where 50% of roots are found (m)\n",
    "    - `root_depth_99`: Soil depth where 99% of roots are found (m)\n",
    "### <span style=\"color:green\">Vegetation Attributes</span>\n",
    "<p style='text-align: justify;'>\n",
    "Following Addor et al. (2017), we extract 13 vegetation characteristics derived from <b>MODIS</b> satellite data (LAI and NDVI) and the <b>NLCD 2021</b> land cover dataset. \n",
    "These attributes capture both the static properties of the land surface (cover type, root depth) and the dynamic seasonal behavior of vegetation (phenology).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_modis_lai(client, watershed_geom, years=[2008, 2015, 2021]):\n",
    "    \"\"\"\n",
    "    Extract LAI statistics from MODIS LAI product using distributed temporal sampling.\n",
    "    \n",
    "    Uses distributed multi-year sampling to compute climatology - same methodology\n",
    "    as production version, just using Planetary Computer instead of GEE.\n",
    "    \n",
    "    Tutorial uses 3 years for speed (3-5 min instead of 5-10 min).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    client : pystac_client.Client\n",
    "        STAC client for Planetary Computer\n",
    "    watershed_geom : shapely.geometry\n",
    "        Watershed boundary (EPSG:4326)\n",
    "    years : list of int\n",
    "        Years for distributed sampling (default: [2008, 2015, 2021])\n",
    "        3 years spanning 13 years - good balance of speed and robustness\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : LAI attributes\n",
    "        - lai_max: Maximum monthly climatology\n",
    "        - lai_min: Minimum monthly climatology\n",
    "        - lai_diff: Seasonal amplitude (max - min)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  - Extracting MODIS LAI from distributed years: {years}\")\n",
    "        \n",
    "        # Store LAI values grouped by month\n",
    "        monthly_values = {month: [] for month in range(1, 13)}\n",
    "        \n",
    "        for year in years:\n",
    "            for month in range(1, 13):\n",
    "                try:\n",
    "                    # Search for scene near mid-month\n",
    "                    search = client.search(\n",
    "                        collections=[\"modis-15A2H-061\"],\n",
    "                        bbox=watershed_geom.bounds,\n",
    "                        datetime=f\"{year}-{month:02d}-10/{year}-{month:02d}-20\"\n",
    "                    )\n",
    "                    items = list(search.get_items())\n",
    "                    \n",
    "                    if not items:\n",
    "                        # Expand search window\n",
    "                        search = client.search(\n",
    "                            collections=[\"modis-15A2H-061\"],\n",
    "                            bbox=watershed_geom.bounds,\n",
    "                            datetime=f\"{year}-{month:02d}-01/{year}-{month:02d}-28\"\n",
    "                        )\n",
    "                        items = list(search.get_items())\n",
    "                    \n",
    "                    if items:\n",
    "                        item = planetary_computer.sign(items[0])\n",
    "                        lai_asset = item.assets[\"Lai_500m\"]\n",
    "                        \n",
    "                        # Download and clip to watershed\n",
    "                        lai = rioxarray.open_rasterio(lai_asset.href, masked=True)\n",
    "                        lai_clipped = lai.rio.clip(\n",
    "                            [watershed_geom], \n",
    "                            crs=\"EPSG:4326\", \n",
    "                            drop=True, \n",
    "                            invert=False\n",
    "                        )\n",
    "                        \n",
    "                        # Apply scale factor and filter\n",
    "                        lai_clipped = lai_clipped * 0.1\n",
    "                        lai_clipped = lai_clipped.where(lai_clipped <= 10)\n",
    "                        \n",
    "                        # Compute spatial mean\n",
    "                        spatial_mean = float(lai_clipped.mean().values)\n",
    "                        \n",
    "                        if not np.isnan(spatial_mean) and spatial_mean > 0:\n",
    "                            monthly_values[month].append(spatial_mean)\n",
    "                \n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        # Compute monthly climatology (average across years for each month)\n",
    "        monthly_climatology = []\n",
    "        for month in range(1, 13):\n",
    "            if monthly_values[month]:\n",
    "                monthly_climatology.append(np.mean(monthly_values[month]))\n",
    "        \n",
    "        if len(monthly_climatology) >= 6:\n",
    "            lai_max = float(np.max(monthly_climatology))\n",
    "            lai_min = float(np.min(monthly_climatology))\n",
    "            lai_diff = lai_max - lai_min\n",
    "            \n",
    "            print(f\"    ✓ LAI climatology: max={lai_max:.2f}, min={lai_min:.2f}, diff={lai_diff:.2f}\")\n",
    "            \n",
    "            return {\n",
    "                \"lai_max\": lai_max,\n",
    "                \"lai_min\": lai_min,\n",
    "                \"lai_diff\": lai_diff\n",
    "            }\n",
    "        else:\n",
    "            print(f\"    ⚠ Insufficient data, using defaults\")\n",
    "            return {\"lai_max\": 3.0, \"lai_min\": 1.0, \"lai_diff\": 2.0}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠ Error extracting LAI: {str(e)}\")\n",
    "        return {\"lai_max\": 3.0, \"lai_min\": 1.0, \"lai_diff\": 2.0}\n",
    "\n",
    "\n",
    "def extract_modis_ndvi(client, watershed_geom, years=[2008, 2015, 2021]):\n",
    "    \"\"\"\n",
    "    Extract NDVI/GVF statistics from MODIS NDVI product using distributed temporal sampling.\n",
    "    \n",
    "    Uses distributed multi-year sampling to compute climatology - same methodology\n",
    "    as production version, just using Planetary Computer instead of GEE.\n",
    "    \n",
    "    Tutorial uses 3 years for speed (3-5 min instead of 5-10 min).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    client : pystac_client.Client\n",
    "        STAC client for Planetary Computer\n",
    "    watershed_geom : shapely.geometry\n",
    "        Watershed boundary (EPSG:4326)\n",
    "    years : list of int\n",
    "        Years for distributed sampling (default: [2008, 2015, 2021])\n",
    "        3 years spanning 13 years - good balance of speed and robustness\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : GVF attributes\n",
    "        - gvf_max: Maximum monthly climatology\n",
    "        - gvf_diff: Seasonal amplitude\n",
    "        - gvf_mean: Annual mean\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  - Extracting MODIS NDVI/GVF from distributed years: {years}\")\n",
    "        \n",
    "        # Store GVF values grouped by month\n",
    "        monthly_values = {month: [] for month in range(1, 13)}\n",
    "        \n",
    "        for year in years:\n",
    "            for month in range(1, 13):\n",
    "                try:\n",
    "                    # Search for scene near mid-month\n",
    "                    search = client.search(\n",
    "                        collections=[\"modis-13Q1-061\"],\n",
    "                        bbox=watershed_geom.bounds,\n",
    "                        datetime=f\"{year}-{month:02d}-10/{year}-{month:02d}-20\"\n",
    "                    )\n",
    "                    items = list(search.get_items())\n",
    "                    \n",
    "                    if not items:\n",
    "                        # Expand search window\n",
    "                        search = client.search(\n",
    "                            collections=[\"modis-13Q1-061\"],\n",
    "                            bbox=watershed_geom.bounds,\n",
    "                            datetime=f\"{year}-{month:02d}-01/{year}-{month:02d}-28\"\n",
    "                        )\n",
    "                        items = list(search.get_items())\n",
    "                    \n",
    "                    if items:\n",
    "                        item = planetary_computer.sign(items[0])\n",
    "                        ndvi_asset = item.assets[\"250m_16_days_NDVI\"]\n",
    "                        \n",
    "                        # Download and clip to watershed\n",
    "                        ndvi = rioxarray.open_rasterio(ndvi_asset.href, masked=True)\n",
    "                        ndvi_clipped = ndvi.rio.clip(\n",
    "                            [watershed_geom], \n",
    "                            crs=\"EPSG:4326\", \n",
    "                            drop=True, \n",
    "                            invert=False\n",
    "                        )\n",
    "                        \n",
    "                        # Convert to GVF (scale factor: divide by 10000)\n",
    "                        gvf = ndvi_clipped / 10000.0\n",
    "                        gvf = gvf.where((gvf >= -1) & (gvf <= 1))\n",
    "                        \n",
    "                        # Compute spatial mean\n",
    "                        spatial_mean = float(gvf.mean().values)\n",
    "                        \n",
    "                        if not np.isnan(spatial_mean):\n",
    "                            monthly_values[month].append(spatial_mean)\n",
    "                \n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        # Compute monthly climatology (average across years for each month)\n",
    "        monthly_climatology = []\n",
    "        for month in range(1, 13):\n",
    "            if monthly_values[month]:\n",
    "                monthly_climatology.append(np.mean(monthly_values[month]))\n",
    "        \n",
    "        if len(monthly_climatology) >= 6:\n",
    "            gvf_max = float(np.max(monthly_climatology))\n",
    "            gvf_min = float(np.min(monthly_climatology))\n",
    "            gvf_diff = gvf_max - gvf_min\n",
    "            gvf_mean = float(np.mean(monthly_climatology))\n",
    "            \n",
    "            print(f\"    ✓ GVF climatology: max={gvf_max:.2f}, diff={gvf_diff:.2f}, mean={gvf_mean:.2f}\")\n",
    "            \n",
    "            return {\n",
    "                \"gvf_max\": gvf_max,\n",
    "                \"gvf_diff\": gvf_diff,\n",
    "                \"gvf_mean\": gvf_mean\n",
    "            }\n",
    "        else:\n",
    "            print(f\"    ⚠ Insufficient data, using defaults\")\n",
    "            return {\"gvf_max\": 0.7, \"gvf_diff\": 0.5, \"gvf_mean\": 0.45}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠ Error extracting NDVI: {str(e)}\")\n",
    "        return {\"gvf_max\": 0.7, \"gvf_diff\": 0.5, \"gvf_mean\": 0.45}\n",
    "\n",
    "\n",
    "def estimate_root_depth(dom_land_cover):\n",
    "    \"\"\"\n",
    "    Estimate root depth from dominant NLCD land cover category.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dom_land_cover : str\n",
    "        NLCD land cover category name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (root_depth_50, root_depth_99) in meters\n",
    "        50th and 99th percentile root depths\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Based on literature values for typical vegetation types:\n",
    "    - Schenk & Jackson (2002): The global biogeography of roots\n",
    "    - Canadell et al. (1996): Maximum rooting depth of vegetation types\n",
    "    \"\"\"\n",
    "    root_depth_lookup = {\n",
    "        \"Forest\": (0.7, 2.0),                      # Deep roots\n",
    "        \"Shrubland\": (0.4, 1.2),                   # Moderate roots\n",
    "        \"Grassland/Herbaceous\": (0.3, 1.0),        # Shallow to moderate\n",
    "        \"Pasture/Hay\": (0.3, 0.8),                 # Shallow roots\n",
    "        \"Planted/Cultivated\": (0.3, 0.8),          # Agricultural crops\n",
    "        \"Woody Wetlands\": (0.2, 0.5),              # Shallow (waterlogged)\n",
    "        \"Emergent Herbaceous Wetlands\": (0.2, 0.5),# Shallow (waterlogged)\n",
    "        \"Water\": (0.0, 0.0),                       # No vegetation\n",
    "        \"Barren\": (0.1, 0.3),                      # Minimal vegetation\n",
    "        \"Developed\": (0.2, 0.6),                   # Mixed urban vegetation\n",
    "    }\n",
    "    \n",
    "    # Return lookup value or default\n",
    "    return root_depth_lookup.get(dom_land_cover, (0.4, 1.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b68b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXTRACTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_vegetation_attributes(watershed_geom, gauge_id, years=[2008, 2015, 2021]):\n",
    "    \"\"\"\n",
    "    Extract CAMELS-style vegetation characteristics using distributed temporal sampling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    watershed_geom : shapely.geometry\n",
    "        Watershed boundary (EPSG:4326)\n",
    "    gauge_id : str\n",
    "        USGS gauge identifier\n",
    "    years : list of int\n",
    "        Years for distributed sampling (default: [2008, 2015, 2021])\n",
    "        Tutorial uses 3 years spanning 13 years for speed\n",
    "        Production can use 5+ years since GEE is much faster\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : CAMELS vegetation attributes\n",
    "        - lai_max: Maximum monthly climatology\n",
    "        - lai_min: Minimum monthly climatology\n",
    "        - lai_diff: Seasonal LAI amplitude\n",
    "        - gvf_max: Maximum monthly climatology\n",
    "        - gvf_diff: Seasonal GVF amplitude\n",
    "        - gvf_mean: Annual mean GVF\n",
    "        - frac_forest: Forest fraction (0-1)\n",
    "        - frac_cropland: Cropland fraction (0-1)\n",
    "        - water_frac: Water fraction (0-1)\n",
    "        - dom_land_cover: Dominant land cover category\n",
    "        - dom_land_cover_frac: Dominant category fraction (0-1)\n",
    "        - root_depth_50: 50th percentile root depth (m)\n",
    "        - root_depth_99: 99th percentile root depth (m)\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"EXTRACTING VEGETATION ATTRIBUTES - Gauge {gauge_id}\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Tutorial version using Planetary Computer\")\n",
    "        print(f\"Distributed years: {years} (spans {max(years)-min(years)} years)\")\n",
    "        print(f\"Expected time: 3-5 minutes\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Initialize Planetary Computer STAC client\n",
    "        print(\"Connecting to Microsoft Planetary Computer...\")\n",
    "        client = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "        print(\"  ✓ Connected\\n\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 1. MODIS LAI (Leaf Area Index) - Distributed Multi-Year\n",
    "        # ====================================================================\n",
    "        print(\"[1/3] MODIS LAI Extraction (Distributed Multi-Year Sampling)\")\n",
    "        lai_stats = extract_modis_lai(client, watershed_geom, years)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 2. MODIS NDVI/GVF (Green Vegetation Fraction) - Distributed Multi-Year\n",
    "        # ====================================================================\n",
    "        print(\"\\n[2/3] MODIS NDVI/GVF Extraction (Distributed Multi-Year Sampling)\")\n",
    "        ndvi_stats = extract_modis_ndvi(client, watershed_geom, years)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 3. NLCD 2021 Land Cover\n",
    "        # ====================================================================\n",
    "        print(\"\\n[3/3] NLCD 2021 Land Cover Extraction\")\n",
    "        print(\"  - Fetching NLCD 2021 data...\")\n",
    "        \n",
    "        # Create GeoDataFrame for pygeohydro\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            index=[str(gauge_id)], \n",
    "            crs=\"EPSG:4326\", \n",
    "            geometry=[watershed_geom]\n",
    "        )\n",
    "        \n",
    "        # Download NLCD land cover data\n",
    "        lulc = gh.nlcd_bygeom(gdf, resolution=30, years={\"cover\": [2021]}, ssl=False)\n",
    "        \n",
    "        # Compute land cover statistics\n",
    "        stats = gh.cover_statistics(lulc[str(gauge_id)].cover_2021)\n",
    "        \n",
    "        # Convert percentages to fractions (0-1)\n",
    "        categories_frac = {k: v / 100.0 for k, v in stats.categories.items()}\n",
    "        \n",
    "        # Extract required attributes\n",
    "        frac_forest = categories_frac.get(\"Forest\", 0.0)\n",
    "        frac_cropland = categories_frac.get(\"Planted/Cultivated\", 0.0)\n",
    "        water_frac = categories_frac.get(\"Water\", 0.0)\n",
    "        \n",
    "        # Find dominant land cover category\n",
    "        dom_land_cover = max(categories_frac, key=categories_frac.get)\n",
    "        dom_land_cover_frac = categories_frac[dom_land_cover]\n",
    "        \n",
    "        print(f\"  ✓ Dominant cover: {dom_land_cover} ({dom_land_cover_frac:.1%})\")\n",
    "        print(f\"  ✓ Forest: {frac_forest:.1%}, Cropland: {frac_cropland:.1%}, Water: {water_frac:.1%}\")\n",
    "        \n",
    "        lc_stats = {\n",
    "            \"frac_forest\": float(frac_forest),\n",
    "            \"frac_cropland\": float(frac_cropland),\n",
    "            \"water_frac\": float(water_frac),\n",
    "            \"dom_land_cover\": dom_land_cover,\n",
    "            \"dom_land_cover_frac\": float(dom_land_cover_frac)\n",
    "        }\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 4. Root Depth Estimation\n",
    "        # ====================================================================\n",
    "        print(\"\\n[4/4] Root Depth Estimation\")\n",
    "        root_depth = estimate_root_depth(dom_land_cover)\n",
    "        print(f\"  ✓ Root depth: 50%={root_depth[0]:.1f}m, 99%={root_depth[1]:.1f}m\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Combine all attributes\n",
    "        # ====================================================================\n",
    "        veg_attrs = {\n",
    "            **lai_stats,\n",
    "            **ndvi_stats,\n",
    "            **lc_stats,\n",
    "            \"root_depth_50\": float(root_depth[0]),\n",
    "            \"root_depth_99\": float(root_depth[1])\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"✓ VEGETATION ATTRIBUTES EXTRACTED SUCCESSFULLY\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        return veg_attrs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error extracting vegetation attributes: {str(e)}\")\n",
    "        print(\"Using default CAMELS vegetation values...\\n\")\n",
    "        \n",
    "        # Return default values\n",
    "        return {\n",
    "            \"lai_max\": 3.0,\n",
    "            \"lai_min\": 1.0,\n",
    "            \"lai_diff\": 2.0,\n",
    "            \"gvf_max\": 0.7,\n",
    "            \"gvf_diff\": 0.5,\n",
    "            \"gvf_mean\": 0.45,\n",
    "            \"frac_forest\": 0.5,\n",
    "            \"frac_cropland\": 0.1,\n",
    "            \"water_frac\": 0.05,\n",
    "            \"dom_land_cover\": \"Forest\",\n",
    "            \"dom_land_cover_frac\": 0.5,\n",
    "            \"root_depth_50\": 0.4,\n",
    "            \"root_depth_99\": 1.0\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    # Extract vegetation attributes\n",
    "    veg_attrs = extract_vegetation_attributes(watershed_geom, gauge_id)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nVegetation Attributes:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in veg_attrs.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key:25s}: {value:.3f}\")\n",
    "        else:\n",
    "            print(f\"  {key:25s}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd4dff",
   "metadata": {},
   "source": [
    "<a id=\"9-geological-characteristics\"></a>\n",
    "## <span style=\"color:green\">9. Geological Characteristics</span>\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Understand geological controls on baseflow and groundwater  \n",
    "- Access global lithological and hydrogeological datasets  \n",
    "- Compute permeability and porosity statistics  \n",
    "\n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align: justify;'>\n",
    "Geology governs the subsurface component of the hydrological cycle. \n",
    "<b>Lithology</b> determines how permeable the subsurface is and how water can move through bedrock. \n",
    "<b>Bedrock permeability</b> controls the generation of baseflow and the rate at which water can recharge aquifers. \n",
    "<b>Geological structure</b> influences groundwater storage and flow pathways, while the presence of <b>carbonate rocks</b> often results in karst features that create distinctive hydrogeological behavior. \n",
    "Together, these attributes capture long-term water availability and storage processes that complement climate and soil drivers.\n",
    "</p>\n",
    "\n",
    "**Attributes Extracted:**\n",
    "- **Lithology**: 1st (`geol_1st_class`) and 2nd (`geol_2nd_class`) dominant classes\n",
    "- **Lithology Fractions**: Fraction of 1st (`glim_1st_class_frac`) and 2nd (`glim_2nd_class_frac`) classes\n",
    "- **Carbonate Rocks**: Fraction of carbonate rock area (`carbonate_rocks_frac`)\n",
    "- **Permeability (`geol_permeability`)**: Log-transformed subsurface permeability — in log10(m²)\n",
    "- **Porosity (`geol_porosity`)**: Subsurface porosity (dimensionless)\n",
    "\n",
    "### <span style=\"color:green\">Geological Analysis Functions</span>\n",
    "<p style='text-align: justify;'>\n",
    "In this step, we integrate <b>GLiM lithology data</b> with <b>GLHYMPS hydrogeological properties</b> to extract geological descriptors at the catchment scale. \n",
    "Key outputs include dominant geologic classes, the fraction of carbonate rocks, subsurface porosity, and log-transformed permeability. \n",
    "These replicate the <b>camels_geol</b> attributes in CAMELS and provide insight into the catchment's ability to store and transmit groundwater.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_geological_attributes(watershed_gdf):\n",
    "    \"\"\"\n",
    "    Extract geological characteristics using GLiM and GLHYMPS datasets\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    geol_attrs : dict\n",
    "        Geological attribute values (filtered)\n",
    "    geol_units : dict\n",
    "        Units for each attribute (filtered)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Extracting geological attributes...\")\n",
    "        \n",
    "        # --- Lithology (GLiM) ---\n",
    "        print(\"  - Processing lithological data (GLiM)...\")\n",
    "        try:\n",
    "            glim_result = glim_attributes(watershed_gdf)\n",
    "            print(f\"    ✓ Found dominant lithology: {glim_result.get('geol_1st_class', 'unknown')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - GLiM data unavailable: {str(e)}\")\n",
    "            glim_result = {\n",
    "                \"geol_1st_class\": \"unknown\",\n",
    "                \"geol_2nd_class\": \"unknown\",\n",
    "                \"glim_1st_class_frac\": 0.5,\n",
    "                \"glim_2nd_class_frac\": 0.3,\n",
    "                \"carbonate_rocks_frac\": 0.0,\n",
    "            }\n",
    "        \n",
    "        # --- Hydrogeology (GLHYMPS) ---\n",
    "        print(\"  - Processing hydrogeological data (GLHYMPS)...\")\n",
    "        try:\n",
    "            glhymps_result = glhymps_attributes(watershed_gdf)\n",
    "            print(f\"    ✓ Mean permeability (log10 m²): {glhymps_result.get('geol_permeability', 'unknown')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - GLHYMPS data unavailable: {str(e)}\")\n",
    "            glhymps_result = {\n",
    "                \"geol_permeability\": -14.0,      # log10(m²)\n",
    "                \"geol_porostiy\": 0.1             # fraction\n",
    "            }\n",
    "\n",
    "        # Merge and filter attributes (exclude linear permeability & hydraulic conductivity)\n",
    "        merged = {**glim_result, **glhymps_result}\n",
    "        geol_attrs = {k: v for k, v in merged.items() if k not in [\"geol_permeability_linear\", \"hydraulic_conductivity\"]}\n",
    "        \n",
    "        # Units\n",
    "        geol_units = {\n",
    "            \"geol_1st_class\": \"\",                      # categorical\n",
    "            \"geol_2nd_class\": \"\",                      # categorical\n",
    "            \"glim_1st_class_frac\": \"fraction\",         # 0–1\n",
    "            \"glim_2nd_class_frac\": \"fraction\",\n",
    "            \"carbonate_rocks_frac\": \"fraction\",\n",
    "            \"geol_permeability\": \"log10(m²)\",          # log permeability\n",
    "            \"geol_porostiy\": \"fraction\",               # 0–1\n",
    "        }\n",
    "\n",
    "        print(\"✓ Geological attributes extracted successfully\")\n",
    "        return geol_attrs, geol_units\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error extracting geological attributes: {str(e)}\")\n",
    "        print(\"Using default geological properties...\")\n",
    "        geol_attrs = {\n",
    "            \"geol_1st_class\": \"mixed\",\n",
    "            \"geol_2nd_class\": \"mixed\",\n",
    "            \"glim_1st_class_frac\": 0.6,\n",
    "            \"glim_2nd_class_frac\": 0.4,\n",
    "            \"carbonate_rocks_frac\": 0.0,\n",
    "            \"geol_permeability\": -14.0,\n",
    "            \"geol_porostiy\": 0.1\n",
    "        }\n",
    "        geol_units = {\n",
    "            \"geol_1st_class\": \"\",\n",
    "            \"geol_2nd_class\": \"\",\n",
    "            \"glim_1st_class_frac\": \"fraction\",\n",
    "            \"glim_2nd_class_frac\": \"fraction\",\n",
    "            \"carbonate_rocks_frac\": \"fraction\",\n",
    "            \"geol_permeability\": \"log10(m²)\",\n",
    "            \"geol_porostiy\": \"fraction\"\n",
    "        }\n",
    "        return geol_attrs, geol_units\n",
    "    \n",
    "\n",
    "\n",
    "# === Extracting geological attributese ===\n",
    "geol_attrs, geol_units = extract_geological_attributes(watershed_gdf)\n",
    "\n",
    "print(\"\\nGeological Attributes:\")\n",
    "for key, value in geol_attrs.items():\n",
    "    unit = geol_units.get(key, \"\")\n",
    "    if isinstance(value, float):\n",
    "        if np.isnan(value):\n",
    "            print(f\"  {key}: NaN {unit}\")\n",
    "        elif \"linear\" in key or \"conductivity\" in key:\n",
    "            print(f\"  {key}: {value:.3e} {unit}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value:.3f} {unit}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value} {unit}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79604d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"10-hydrological-signatures\"></a>\n",
    "## <span style=\"color:green\">10. Hydrological Signatures</span>\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Understand how streamflow reflects catchment function  \n",
    "- Compute hydrological signatures from discharge and precipitation  \n",
    "- Interpret statistical, event-based, and timing-based indicators  \n",
    "- Apply water balance analysis using remotely sensed climate data  \n",
    "\n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align: justify;'>\n",
    "Hydrological signatures are quantitative descriptors of catchment behavior, capturing how precipitation is transformed into streamflow across different temporal and spatial scales. They summarize flow regimes by integrating the combined effects of <b>climate, soils, topography, vegetation, and geology</b>.\n",
    "</p>\n",
    "\n",
    "**Key Categories of Signatures:**\n",
    "- <b>Flow Statistics</b>: Central tendency and variability of discharge (mean, median, Q5, Q95, standard deviation).  \n",
    "- <b>Water Balance</b>: Partitioning of precipitation into streamflow (runoff ratio) and sensitivity of flow to rainfall variability (streamflow elasticity).  \n",
    "- <b>Event Statistics</b>: Frequency and duration of extreme hydrological events, including floods (high-flow) and droughts (low-flow or zero-flow).  \n",
    "- <b>Timing Statistics</b>: Seasonal metrics such as the <b>half-flow date</b> (day of year when 50% of annual flow has passed) and its variability across years.  \n",
    "\n",
    "**Signatures Computed:**\n",
    "- **Flow Statistics**: Mean (`q_mean`), Median (`q_median`), Std Dev (`q_std`), Low Flow (`q95`), High Flow (`q5`) — in mm/day\n",
    "- **Baseflow Index (`baseflow_index`)**: Ratio of baseflow to total flow (dimensionless)\n",
    "- **Runoff Ratio (`runoff_ratio`)**: Ratio of mean discharge to mean precipitation (dimensionless)\n",
    "- **Elasticity (`stream_elas`)**: Sensitivity of streamflow to precipitation changes (dimensionless)\n",
    "- **High Flow**: Frequency (`high_q_freq`, events/yr) and Duration (`high_q_dur`, days)\n",
    "- **Low Flow**: Frequency (`low_q_freq`, events/yr) and Duration (`low_q_dur`, days)\n",
    "- **Zero Flow**: Frequency of zero-flow days (`zero_q_freq`, dimensionless)\n",
    "- **Timing**: Mean Half-Flow Date (`hfd_mean`, day-of-hydrologic-year) and its variability (`half_flow_date_std`, days)\n",
    "- **FDC Slope (`slope_fdc`)**: Slope of Flow Duration Curve in log space\n",
    "\n",
    "**Data Sources Used:**\n",
    "- <b>Streamflow</b>: USGS NWIS daily discharge records  \n",
    "- <b>Precipitation</b>: GridMET daily precipitation averaged over the watershed  \n",
    "\n",
    "\n",
    "### <span style=\"color:green\">Hydrological Analysis Functions</span>\n",
    "<p style='text-align: justify;'>\n",
    "In this step, we compute CAMELS-style hydrological signatures using combined discharge and precipitation time series. The functions calculate flow statistics, water balance metrics, event frequencies, and timing indices that serve as reproducible descriptors of watershed hydrology.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06f408-0942-487c-98c5-82e4bc57f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Utilities Hydrological Sign.\n",
    "# ------------------------------\n",
    "\n",
    "def _to_mm_per_day(discharge_cms: pd.Series, area_km2: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert discharge from m³/s to basin-depth mm/day using area (km²).\n",
    "    Formula: q_mm_day = q_cms * 86.4 / area_km2\n",
    "    \"\"\"\n",
    "    if discharge_cms is None or len(discharge_cms) == 0 or area_km2 <= 0:\n",
    "        return pd.Series(dtype=float)\n",
    "    q_mm_day = discharge_cms * (86.4 / area_km2)\n",
    "    q_mm_day.index = pd.to_datetime(q_mm_day.index).tz_localize(None)\n",
    "    return q_mm_day\n",
    "\n",
    "\n",
    "def _align_daily(q_series: pd.Series, p_series: pd.Series, min_days: int = 365):\n",
    "    \"\"\"\n",
    "    Align to common daily index (naive datetime), drop NaNs, require minimum overlap.\n",
    "    Returns q_aligned, p_aligned (both Series) or (None, None) if insufficient.\n",
    "    \"\"\"\n",
    "    if q_series is None or p_series is None:\n",
    "        return None, None\n",
    "    qi = pd.to_datetime(q_series.index).tz_localize(None)\n",
    "    pi = pd.to_datetime(p_series.index).tz_localize(None)\n",
    "    q = pd.Series(q_series.values, index=qi, dtype=float).dropna()\n",
    "    p = pd.Series(p_series.values, index=pi, dtype=float).dropna()\n",
    "    common = q.index.intersection(p.index)\n",
    "    if len(common) < min_days:\n",
    "        return None, None\n",
    "    return q.loc[common], p.loc[common]\n",
    "\n",
    "\n",
    "def _year_series(dti: pd.DatetimeIndex, hydro_year_start_month=10) -> np.ndarray:\n",
    "    \"\"\"Hydrologic year ID (integer). October=10 for CONUS (CAMELS).\"\"\"\n",
    "    return dti.year + (dti.month >= hydro_year_start_month)\n",
    "\n",
    "\n",
    "def _lyne_hollick_baseflow(q: np.ndarray, alpha: float = 0.925, passes: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Lyne–Hollick digital filter (Ladson et al., 2013). Non-negative constraint.\n",
    "    Returns baseflow array (same length as q).\n",
    "    \"\"\"\n",
    "    if q.size == 0 or np.all(~np.isfinite(q)):\n",
    "        return np.full_like(q, np.nan, dtype=float)\n",
    "\n",
    "    def one_pass_forward(x):\n",
    "        y = np.zeros_like(x, dtype=float)\n",
    "        y[0] = x[0]\n",
    "        for t in range(1, len(x)):\n",
    "            y[t] = alpha * y[t-1] + (1 + alpha) / 2 * (x[t] - x[t-1])\n",
    "            y[t] = min(max(y[t], 0.0), x[t])\n",
    "        return y\n",
    "\n",
    "    def one_pass_backward(x):\n",
    "        y = np.zeros_like(x, dtype=float)\n",
    "        y[-1] = x[-1]\n",
    "        for t in range(len(x) - 2, -1, -1):\n",
    "            y[t] = alpha * y[t+1] + (1 + alpha) / 2 * (x[t] - x[t+1])\n",
    "            y[t] = min(max(y[t], 0.0), x[t])\n",
    "        return y\n",
    "\n",
    "    bf = q.copy().astype(float)\n",
    "    for _ in range(passes):\n",
    "        bf = one_pass_forward(bf)\n",
    "        bf = one_pass_backward(bf)\n",
    "    return np.clip(bf, 0, q)\n",
    "\n",
    "\n",
    "def _consecutive_event_lengths(mask: np.ndarray) -> list:\n",
    "    \"\"\"Given a 0/1 mask, return lengths of consecutive 1's runs.\"\"\"\n",
    "    lengths, run = [], 0\n",
    "    for v in mask:\n",
    "        if v:\n",
    "            run += 1\n",
    "        elif run > 0:\n",
    "            lengths.append(run)\n",
    "            run = 0\n",
    "    if run > 0:\n",
    "        lengths.append(run)\n",
    "    return lengths\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Data fetchers\n",
    "# ------------------------------\n",
    "\n",
    "def fetch_streamflow_data(gauge_id: str, start_date: str, end_date: str) -> Union[pd.Series, None]:\n",
    "    \"\"\"Fetch daily streamflow (USGS NWIS), returns m³/s as a pandas Series (index: date).\"\"\"\n",
    "    try:\n",
    "        nwis = hf.NWIS(gauge_id, \"dv\", start_date, end_date)\n",
    "        df = nwis.df()\n",
    "        if df.empty:\n",
    "            return None\n",
    "        q_cfs = pd.to_numeric(df.iloc[:, 0], errors=\"coerce\")\n",
    "        q_cms = q_cfs * 0.0283168\n",
    "        q_cms.index = pd.to_datetime(q_cms.index).tz_localize(None)\n",
    "        return q_cms.dropna()\n",
    "    except Exception as e:\n",
    "        print(f\"    Error fetching streamflow: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_precipitation_data_bygeom(watershed_geom, start_date: str, end_date: str) -> Union[pd.Series, None]:\n",
    "    \"\"\"GridMET watershed-average precipitation (mm/day) as a daily Series.\"\"\"\n",
    "    try:\n",
    "        ds = gridmet.get_bygeom(\n",
    "            geometry=watershed_geom,\n",
    "            dates=(start_date, end_date),\n",
    "            variables=[\"pr\"],\n",
    "            crs=\"EPSG:4326\",\n",
    "        )\n",
    "        if \"pr\" not in ds.data_vars:\n",
    "            raise ValueError(f\"'pr' not in dataset vars: {list(ds.data_vars)}\")\n",
    "        pr_daily = ds[\"pr\"].mean(dim=[\"lat\", \"lon\"])  # mm/day\n",
    "        s = pr_daily.to_series().dropna()\n",
    "        s.index = pd.to_datetime(s.index).tz_localize(None)\n",
    "        s.name = \"precip_mm\"\n",
    "        return s\n",
    "    except Exception as e:\n",
    "        print(f\"    Error fetching GridMET precipitation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# CAMELS-consistent signatures\n",
    "# ------------------------------\n",
    "\n",
    "def compute_flow_stats_camels(q_mm_day: pd.Series) -> dict:\n",
    "    \"\"\"Flow-based stats: q_mean, q_std, Q5, Q95, q_median, baseflow_index.\"\"\"\n",
    "    q = q_mm_day.dropna().values\n",
    "    if len(q) < 365:\n",
    "        return {k: np.nan for k in [\"q_mean\",\"q_std\",\"q5\",\"q95\",\"q_median\",\"baseflow_index\"]}\n",
    "\n",
    "    q_mean = float(np.mean(q))\n",
    "    q_std  = float(np.std(q))\n",
    "    q5     = float(np.quantile(q, 0.95))  # CAMELS: high flow\n",
    "    q95    = float(np.quantile(q, 0.05))  # CAMELS: low flow\n",
    "    q_med  = float(np.median(q))\n",
    "\n",
    "    bf = _lyne_hollick_baseflow(q, alpha=0.925, passes=3)\n",
    "    bfi = float(np.nansum(bf) / np.nansum(q)) if np.nansum(q) > 0 else np.nan\n",
    "    bfi = max(0.0, min(1.0, bfi)) if np.isfinite(bfi) else np.nan\n",
    "\n",
    "    return dict(q_mean=q_mean, q_std=q_std, q5=q5, q95=q95,\n",
    "                q_median=q_med, baseflow_index=bfi)\n",
    "\n",
    "\n",
    "def compute_water_balance_camels(q_mm_day: pd.Series, p_mm_day: pd.Series) -> dict:\n",
    "    \"\"\"Runoff ratio and streamflow elasticity.\"\"\"\n",
    "    q_aln, p_aln = _align_daily(q_mm_day, p_mm_day, min_days=365)\n",
    "    if q_aln is None:\n",
    "        return dict(runoff_ratio=np.nan, stream_elas=np.nan)\n",
    "\n",
    "    mean_q, mean_p = float(q_aln.mean()), float(p_aln.mean())\n",
    "    rr = mean_q / mean_p if mean_p > 0 else np.nan\n",
    "\n",
    "    hy = _year_series(q_aln.index, hydro_year_start_month=10)\n",
    "    mp = pd.Series(p_aln.values, index=hy).groupby(level=0).mean()\n",
    "    mq = pd.Series(q_aln.values, index=hy).groupby(level=0).mean()\n",
    "\n",
    "    if len(mp) < 3 or len(mq) < 3:\n",
    "        return dict(runoff_ratio=rr, stream_elas=np.nan)\n",
    "\n",
    "    mp_tot, mq_tot = float(mp.mean()), float(mq.mean())\n",
    "    dp, dq = (mp - mp_tot), (mq - mq_tot)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ratio = (dq / mq_tot) / (dp / mp_tot)\n",
    "    ratio = ratio.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    elas = float(np.median(ratio)) if len(ratio) > 0 else np.nan\n",
    "\n",
    "    return dict(runoff_ratio=rr, stream_elas=elas)\n",
    "\n",
    "\n",
    "def compute_event_stats_camels(q_mm_day: pd.Series) -> dict:\n",
    "    \"\"\"High/low flow frequencies and durations, zero_q_freq, flow_variability.\"\"\"\n",
    "    q = q_mm_day.dropna().values\n",
    "    if len(q) == 0:\n",
    "        return {k: np.nan for k in\n",
    "                [\"high_q_freq\",\"high_q_dur\",\"low_q_freq\",\"low_q_dur\",\"zero_q_freq\",\"flow_variability\"]}\n",
    "\n",
    "    med_q, mean_q = np.median(q), np.mean(q)\n",
    "    if mean_q <= 0:\n",
    "        return {k: np.nan for k in\n",
    "                [\"high_q_freq\",\"high_q_dur\",\"low_q_freq\",\"low_q_dur\",\"zero_q_freq\",\"flow_variability\"]}\n",
    "\n",
    "    high_mask = (q > 9.0 * med_q)\n",
    "    high_freq = float(np.sum(high_mask) / len(q) * 365.25)\n",
    "    high_dur  = float(np.mean(_consecutive_event_lengths(high_mask))) if np.any(high_mask) else np.nan\n",
    "\n",
    "    low_mask = (q <= 0.2 * mean_q)\n",
    "    low_freq = float(np.sum(low_mask) / len(q) * 365.25)\n",
    "    low_dur  = float(np.mean(_consecutive_event_lengths(low_mask))) if np.any(low_mask) else np.nan\n",
    "\n",
    "    zero_freq = float(np.sum(q == 0) / len(q))\n",
    "    flow_var  = float(np.std(q) / mean_q)\n",
    "\n",
    "    return dict(high_q_freq=high_freq, high_q_dur=high_dur,\n",
    "                low_q_freq=low_freq, low_q_dur=low_dur,\n",
    "                zero_q_freq=zero_freq, flow_variability=flow_var)\n",
    "\n",
    "\n",
    "def compute_timing_stats_camels(q_mm_day: pd.Series) -> dict:\n",
    "    \"\"\"Half-flow date mean/std by hydrologic year (Oct start).\"\"\"\n",
    "    if q_mm_day is None or len(q_mm_day) < 365:\n",
    "        return dict(hfd_mean=np.nan, half_flow_date_std=np.nan)\n",
    "\n",
    "    df = q_mm_day.dropna().to_frame(\"q\")\n",
    "    hy = _year_series(df.index, hydro_year_start_month=10)\n",
    "    hyd_start = pd.to_datetime([f\"{y-1}-10-01\" for y in hy])\n",
    "    df[\"day\"] = (df.index - hyd_start).days + 1\n",
    "    df[\"hy\"] = hy\n",
    "\n",
    "    hfd_list = []\n",
    "    for g, grp in df.groupby(\"hy\"):\n",
    "        qsum = grp[\"q\"].sum()\n",
    "        if len(grp) >= 300 and qsum > 0:\n",
    "            csum = grp[\"q\"].cumsum()\n",
    "            idx = np.argmax(csum.values >= 0.5 * qsum)\n",
    "            hfd_list.append(int(grp[\"day\"].iloc[idx]))\n",
    "\n",
    "    if len(hfd_list) >= 2:\n",
    "        return dict(hfd_mean=float(np.mean(hfd_list)),\n",
    "                    half_flow_date_std=float(np.std(hfd_list)))\n",
    "    return dict(hfd_mean=np.nan, half_flow_date_std=np.nan)\n",
    "\n",
    "\n",
    "def compute_slope_fdc_camels(q_mm_day: pd.Series) -> dict:\n",
    "    \"\"\"Slope of FDC in log space between 33% and 66% exceedance (Sawicz 2011).\"\"\"\n",
    "    q = q_mm_day.dropna().values\n",
    "    q = q[q > 0]\n",
    "    if len(q) < 100:\n",
    "        return dict(slope_fdc=np.nan)\n",
    "    q33, q66 = np.quantile(q, 0.67), np.quantile(q, 0.34)\n",
    "    if q66 <= 0 or q33 <= 0:\n",
    "        return dict(slope_fdc=np.nan)\n",
    "    slope = (np.log(q33) - np.log(q66)) / (0.66 - 0.33)\n",
    "    return dict(slope_fdc=float(slope))\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Main extraction function\n",
    "# ------------------------------\n",
    "\n",
    "def extract_hydrological_signatures(\n",
    "    gauge_id: str,\n",
    "    watershed_gdf,\n",
    "    start_date: str = \"1989-10-01\",\n",
    "    end_date: str = \"2009-09-30\",\n",
    ") -> dict:\n",
    "    \"\"\"CAMELS-consistent hydrological signatures (units in mm/day).\"\"\"\n",
    "    try:\n",
    "        print(\"Extracting hydrological signatures ...\")\n",
    "        #area_km2 = watershed_gdf.to_crs(\"EPSG:5070\").geometry.area.iloc[0] / 1e6\n",
    "        if not np.isfinite(area_km2) or area_km2 <= 0:\n",
    "            raise ValueError(\"Invalid basin area for conversion to mm/day.\")\n",
    "\n",
    "        q_cms = fetch_streamflow_data(gauge_id, start_date, end_date)\n",
    "        if q_cms is None or len(q_cms) < 365:\n",
    "            print(\"  - Insufficient streamflow data.\")\n",
    "            return {k: np.nan for k in [\n",
    "                \"q_mean\",\"q_std\",\"q5\",\"q95\",\"q_median\",\"baseflow_index\",\n",
    "                \"runoff_ratio\",\"stream_elas\",\"high_q_freq\",\"high_q_dur\",\n",
    "                \"low_q_freq\",\"low_q_dur\",\"zero_q_freq\",\"flow_variability\",\n",
    "                \"hfd_mean\",\"half_flow_date_std\",\"slope_fdc\"\n",
    "            ]}\n",
    "        q_mm_day = _to_mm_per_day(q_cms, area_km2)\n",
    "        p_mm_day = fetch_precipitation_data_bygeom(watershed_geom, start_date, end_date)\n",
    "\n",
    "        flow_stats   = compute_flow_stats_camels(q_mm_day)\n",
    "        water_balance= compute_water_balance_camels(q_mm_day, p_mm_day)\n",
    "        event_stats  = compute_event_stats_camels(q_mm_day)\n",
    "        timing_stats = compute_timing_stats_camels(q_mm_day)\n",
    "        fdc_stat     = compute_slope_fdc_camels(q_mm_day)\n",
    "\n",
    "        out = {**flow_stats, **water_balance, **event_stats, **timing_stats, **fdc_stat}\n",
    "        print(\"✓ Hydrological signatures computed.\")\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        return {k: np.nan for k in [\n",
    "            \"q_mean\",\"q_std\",\"q5\",\"q95\",\"q_median\",\"baseflow_index\",\n",
    "            \"runoff_ratio\",\"stream_elas\",\"high_q_freq\",\"high_q_dur\",\n",
    "            \"low_q_freq\",\"low_q_dur\",\"zero_q_freq\",\"flow_variability\",\n",
    "            \"hfd_mean\",\"half_flow_date_std\",\"slope_fdc\"\n",
    "        ]}\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Extraction signatures\n",
    "# ------------------------------\n",
    "\n",
    "start_date = \"1989-10-01\"\n",
    "end_date   = \"2009-09-30\"\n",
    "hydro_attrs = extract_hydrological_signatures(gauge_id, watershed_gdf, start_date, end_date)\n",
    "\n",
    "units_map = {\n",
    "    \"q_mean\": \"mm/day\",\"q_std\": \"mm/day\",\"q5\": \"mm/day\",\"q95\": \"mm/day\",\"q_median\": \"mm/day\",\n",
    "    \"baseflow_index\": \"–\",\"runoff_ratio\": \"–\",\"stream_elas\": \"–\",\n",
    "    \"high_q_freq\": \"events/yr\",\"high_q_dur\": \"days\",\"low_q_freq\": \"events/yr\",\"low_q_dur\": \"days\",\n",
    "    \"zero_q_freq\": \"–\",\"flow_variability\": \"–\",\n",
    "    \"hfd_mean\": \"day-of-hydrologic-year\",\"half_flow_date_std\": \"days\",\"slope_fdc\": \"log-slope\"\n",
    "}\n",
    "\n",
    "print(\"\\nHydrological Signatures (CAMELS-consistent):\")\n",
    "for k, v in hydro_attrs.items():\n",
    "    u = units_map.get(k, \"\")\n",
    "    if isinstance(v, (float, int)) and np.isfinite(v):\n",
    "        print(f\"  {k}: {v:.3f} {u}\")\n",
    "    else:\n",
    "        print(f\"  {k}: N/A {u}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b82dd",
   "metadata": {},
   "source": [
    "<a id=\"11-data-integration-and-export\"></a>\n",
    "## <span style=\"color:green\">11. Data Integration and Export</span>\n",
    "\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Combine all extracted attribute categories (topography, climate, soil, vegetation, geology, hydrology) into a single CAMELS-like dataset  \n",
    "- Export results in multiple interoperable formats (CSV, JSON, GeoJSON)  \n",
    "- Generate metadata and human-readable summary documentation for transparency and reproducibility  \n",
    "\n",
    "### <span style=\"color:green\">Hydrological Background</span>\n",
    "<p style='text-align: justify;'>\n",
    "Integrating diverse catchment attributes into a unified dataset enables robust <b>large-sample hydrology</b>. By standardizing attributes from topography, climate, soils, vegetation, geology, and hydrology into a consistent schema, we create a dataset that is directly comparable across basins. \n",
    "This harmonization supports <b>cross-basin comparative studies</b>, improves reproducibility, and provides inputs ready for both <b>process-based hydrological models</b> (e.g., SWAT, VIC) and <b>data-driven machine learning frameworks</b>.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Exporting in multiple formats ensures interoperability:  \n",
    "- <b>CSV/JSON</b> → tabular formats for research workflows and ML pipelines  \n",
    "- <b>GeoJSON</b> → geospatial format for visualization and GIS integration  \n",
    "- <b>Metadata</b> → human-readable documentation of attribute definitions, sources, and references for transparency  \n",
    "</p>\n",
    "\n",
    "### <span style=\"color:green\">Data Integration Function</span>\n",
    "<p style='text-align: justify;'>\n",
    "In this step, we merge attribute dictionaries into a structured dataset and export them to multiple formats. \n",
    "This provides a reproducible CAMELS-like database for any USGS basin, complete with metadata for clarity and future extension.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27beffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_all_attributes(gauge_id, topo_attrs, climate_attrs, soil_attrs, \n",
    "                             veg_attrs, geol_attrs, hydro_attrs, metadata=None):\n",
    "    \"\"\"\n",
    "    Integrate all attributes into a single comprehensive dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gauge_id : str\n",
    "        USGS gauge identifier\n",
    "    *_attrs : dict\n",
    "        Attribute dictionaries from each category\n",
    "    metadata : dict, optional\n",
    "        Gauge metadata (gauge_name, lat, lon, huc_02, etc.)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame : Integrated attribute dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Integrating all attributes...\")\n",
    "        \n",
    "        # Create base information\n",
    "        base_attrs = {\n",
    "            'gauge_id': gauge_id,\n",
    "            'extraction_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "            #'data_version': '1.0'\n",
    "        }\n",
    "        \n",
    "        # Add metadata if available\n",
    "        meta_attrs = metadata if metadata is not None else {}\n",
    "        \n",
    "        # Combine all attributes\n",
    "        all_attrs = {\n",
    "            **base_attrs,\n",
    "            **meta_attrs,\n",
    "            **topo_attrs,\n",
    "            **climate_attrs,\n",
    "            **soil_attrs,\n",
    "            **veg_attrs,\n",
    "            **geol_attrs,\n",
    "            **hydro_attrs\n",
    "        }\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([all_attrs])\n",
    "        \n",
    "        # Add attribute categories for organization\n",
    "        attr_categories = create_attribute_categories()\n",
    "        \n",
    "        print(f\"✓ Integrated {len(all_attrs)} attributes for gauge {gauge_id}\")\n",
    "        return df, attr_categories\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error integrating attributes: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def create_attribute_categories():\n",
    "    \"\"\"Define attribute categories for documentation\"\"\"\n",
    "    return {\n",
    "        'identification': ['gauge_id', 'extraction_date', 'data_version', 'gauge_name', 'gauge_lat', 'gauge_lon',  'geometry'],\n",
    "        'topography': ['elev_mean', 'elev_min', 'elev_max', 'elev_std', 'slope_mean', 'slope_std', 'slope_fdc', 'area_geospa_fabric', 'drainage_density'],\n",
    "        'climate': ['p_mean', 'pet_mean', 'temp_mean', 'aridity', 'p_seasonality', 'temp_seasonality', 'frac_snow', 'high_prec_freq', 'low_prec_freq', 'prec_intensity'],\n",
    "        'soil': ['soil_porosity', 'available_water_capacity', 'field_capacity', 'sand_frac', 'silt_frac', 'clay_frac', 'soil_depth_statsgo', 'max_water_content', 'soil_conductivity'],\n",
    "        'vegetation': ['lai_max', 'lai_min', 'lai_diff', 'gvf_min', 'gvf_max', 'gvf_diff', 'gvf_mean', 'frac_forest', 'frac_cropland', 'water_frac', 'dom_land_cover', 'dom_land_cover_frac', 'root_depth_50', 'root_depth_99'],\n",
    "        'geology': ['geol_1st_class', 'geol_2nd_class', 'glim_1st_class_frac', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porosity', 'geol_permeability'],\n",
    "        'hydrology': ['q_mean', 'q_std', 'q5', 'q95', 'q_median', 'baseflow_index', 'runoff_ratio', 'stream_elas', 'high_q_freq', 'low_q_freq', 'zero_q_freq', 'flow_variability', 'hfd_mean', 'half_flow_date_std']\n",
    "    }\n",
    "\n",
    "\n",
    "def make_json_safe(obj):\n",
    "    \"\"\"Recursively convert objects into JSON-safe types.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): make_json_safe(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple, set)):\n",
    "        return [make_json_safe(v) for v in obj]\n",
    "    elif hasattr(obj, \"item\"):  # numpy scalar\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, (np.generic, np.ndarray)):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def create_metadata(gauge_id, attr_categories, df):\n",
    "    \"\"\"Create comprehensive metadata with safe JSON serialization\"\"\"\n",
    "    quality_flags = assess_data_quality(df)\n",
    "    quality_flags = make_json_safe(quality_flags)\n",
    "    attr_categories = make_json_safe(attr_categories)\n",
    "\n",
    "    metadata = {\n",
    "        \"dataset_info\": {\n",
    "            \"name\": f\"CAMELS-like Dataset for USGS Gauge {gauge_id}\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"creation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"description\": \"Catchment attributes extracted for large-sample hydrology studies\",\n",
    "            \"gauge_id\": str(gauge_id),\n",
    "            \"total_attributes\": int(len(df.columns) - 3),\n",
    "        },\n",
    "        \"attribute_categories\": attr_categories,\n",
    "        \"data_sources\": {\n",
    "            \"topography\": \"USGS 3DEP (30m DEM)\",\n",
    "            \"climate\": \"GridMET (1979-present)\",\n",
    "            \"soil\": \"ScienceBase/gNATSGO / POLARIS\",\n",
    "            \"vegetation\": \"MODIS LAI/NDVI, ESA WorldCover\",\n",
    "            \"geology\": \"GLiM, GLHYMPS\",\n",
    "            \"hydrology\": \"USGS NWIS, GridMET\",\n",
    "        },\n",
    "        \"processing_info\": {\n",
    "            \"watershed_delineation\": \"NLDI web service\",\n",
    "            \"spatial_resolution\": \"Variable by data source\",\n",
    "            \"temporal_coverage\": \"2000-2020 (varies by dataset)\",\n",
    "            \"coordinate_system\": \"WGS84 (EPSG:4326)\",\n",
    "        },\n",
    "        \"quality_flags\": quality_flags,\n",
    "        \"citation\": \"Generated using CAMELS-like dataset tutorial (Galib & Merwade, 2024)\",\n",
    "        \"contact\": \"mgalib@purdue.edu\",\n",
    "    }\n",
    "\n",
    "    return make_json_safe(metadata)\n",
    "\n",
    "def export_results(df, attr_categories, gauge_id):\n",
    "    \"\"\"Export results in multiple formats\"\"\"\n",
    "    try:\n",
    "        print(\"Exporting results...\")\n",
    "\n",
    "        # --- Export attributes (drop geometry if present) ---\n",
    "        df_no_geom = df.drop(columns=[\"geometry\"], errors=\"ignore\")\n",
    "\n",
    "        # CSV\n",
    "        csv_path = f\"outputs/data/camels_attributes_{gauge_id}.csv\"\n",
    "        df_no_geom.to_csv(csv_path, index=False)\n",
    "        print(f\"✓ Exported CSV: {csv_path}\")\n",
    "\n",
    "        # JSON\n",
    "        json_path = f\"outputs/data/camels_attributes_{gauge_id}.json\"\n",
    "        df_no_geom.to_json(json_path, orient=\"records\", indent=2, default_handler=str)\n",
    "        print(f\"✓ Exported JSON: {json_path}\")\n",
    "\n",
    "        # --- Export geometry separately (GeoJSON) ---\n",
    "        if \"geometry\" in df.columns:\n",
    "            gdf = gpd.GeoDataFrame(df[[\"gauge_id\", \"geometry\"]], crs=\"EPSG:4326\")\n",
    "            geojson_path = f\"outputs/data/watershed_{gauge_id}.geojson\"\n",
    "            gdf.to_file(geojson_path, driver=\"GeoJSON\")\n",
    "            print(f\"✓ Exported GeoJSON: {geojson_path}\")\n",
    "\n",
    "        # --- Metadata ---\n",
    "        metadata = create_metadata(gauge_id, attr_categories, df_no_geom)\n",
    "        metadata_path = f\"outputs/data/metadata_{gauge_id}.json\"\n",
    "\n",
    "        import json\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        print(f\"✓ Exported metadata: {metadata_path}\")\n",
    "\n",
    "        # --- Summary report ---\n",
    "        create_summary_report(df_no_geom, gauge_id, attr_categories)\n",
    "\n",
    "        print(\"✓ All results exported successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"✗ Error exporting results: {str(e)}\")\n",
    "        print(\"\\n--- DEBUG TRACEBACK ---\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"Assess data quality and provide quality flags\"\"\"\n",
    "    quality_flags = {}\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    quality_flags['missing_values'] = int(missing_count)\n",
    "    quality_flags['completeness'] = f\"{(1 - missing_count/df.size)*100:.1f}%\"\n",
    "    \n",
    "    # Check for reasonable value ranges\n",
    "    quality_flags['extreme_values'] = []\n",
    "    \n",
    "    # Check elevation (should be reasonable for continental US)\n",
    "    if 'elev_mean' in df.columns:\n",
    "        elev = df['elev_mean'].iloc[0]\n",
    "        if elev < -100 or elev > 4500:\n",
    "            quality_flags['extreme_values'].append(f\"Elevation: {elev}\")\n",
    "    \n",
    "    # Check aridity (should be positive)\n",
    "    if 'aridity' in df.columns:\n",
    "        aridity = df['aridity'].iloc[0]\n",
    "        if aridity < 0 or aridity > 10:\n",
    "            quality_flags['extreme_values'].append(f\"Aridity: {aridity}\")\n",
    "    \n",
    "    return quality_flags\n",
    "\n",
    "def create_summary_report(df, gauge_id, attr_categories):\n",
    "    \"\"\"Create a human-readable summary report\"\"\"\n",
    "    report_path = f\"outputs/data/summary_report_{gauge_id}.txt\"\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(f\"CAMELS-like Dataset Summary Report\\n\")\n",
    "        f.write(f\"{'='*50}\\n\\n\")\n",
    "        f.write(f\"Gauge ID: {gauge_id}\\n\")\n",
    "        f.write(f\"Generation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total Attributes: {len(df.columns)-3}\\n\\n\")\n",
    "        \n",
    "        # Write attributes by category\n",
    "        for category, attributes in attr_categories.items():\n",
    "            f.write(f\"{category.upper()} ATTRIBUTES:\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            for attr in attributes:\n",
    "                if attr in df.columns:\n",
    "                    value = df[attr].iloc[0]\n",
    "                    if isinstance(value, float):\n",
    "                        if np.isnan(value):\n",
    "                            f.write(f\"  {attr}: N/A\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"  {attr}: {value:.3f}\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"  {attr}: {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"✓ Created summary report: {report_path}\")\n",
    "\n",
    "# Integrate and export all results\n",
    "if all([topo_attrs, climate_attrs, soil_attrs, veg_attrs, geol_attrs, hydro_attrs]):\n",
    "    # Make sure metadata is fetched before this (from get_gauge_metadata)\n",
    "    final_df, categories = integrate_all_attributes(\n",
    "        gauge_id,\n",
    "        topo_attrs,\n",
    "        climate_attrs,\n",
    "        soil_attrs,\n",
    "        veg_attrs,\n",
    "        geol_attrs,\n",
    "        hydro_attrs,\n",
    "        metadata=metadata   # ✅ include metadata here\n",
    "    )\n",
    "    \n",
    "    if final_df is not None:\n",
    "        print(f\"\\nFinal dataset shape: {final_df.shape}\")\n",
    "        print(\"\\nFirst few attributes:\")\n",
    "        print(final_df.T.head(10))\n",
    "        \n",
    "        # Export results\n",
    "        export_results(final_df, categories, gauge_id)\n",
    "    else:\n",
    "        print(\"Failed to integrate attributes\")\n",
    "else:\n",
    "    print(\"Some attribute categories are missing. Cannot proceed with integration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ddd7e",
   "metadata": {},
   "source": [
    "<a id=\"12-results-validation\"></a>\n",
    "## <span style=\"color:green\">12. Results Validation</span>\n",
    "\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Compare extracted CAMELS-like attributes with the official CAMELS dataset  \n",
    "- Visualize absolute and relative (percent) differences  \n",
    "- Identify which attributes align well and which diverge significantly  \n",
    "\n",
    "### <span style=\"color:green\">Validation Plots</span>\n",
    "<p style='text-align: justify;'>\n",
    "To ensure reliability, we validate our extracted attributes by directly comparing them against the official CAMELS dataset for the same USGS gauges. \n",
    "This comparison highlights consistency, reveals systematic biases, and ensures that derived metrics follow CAMELS conventions. \n",
    "Two complementary diagnostic plots are generated:\n",
    "</p>\n",
    "\n",
    "1. <b>Absolute Differences (Top Plot):</b>  \n",
    "   Displays the raw magnitude of differences between extracted values and CAMELS reference values.  \n",
    "   Useful for detecting mismatches due to unit conversions (e.g., soil depth in meters vs. millimeters) or calculation methods.  \n",
    "\n",
    "2. <b>Percent Differences (Bottom Plot):</b>  \n",
    "   Shows deviations relative to the CAMELS values.  \n",
    "   Highlights proportional mismatches, even when absolute values are small (e.g., precipitation seasonality, aridity index).  \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Together, these plots provide a comprehensive validation framework, helping identify which attributes align closely with CAMELS standards and which require refinement. \n",
    "This step ensures that the CAMELS-like dataset is both <b>scientifically credible</b> and <b>methodologically transparent</b>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygeohydro import get_camels\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load CAMELS dataset\n",
    "camels_gdf, camels_ds = get_camels()\n",
    "\n",
    "# Step 2: Normalize gauge_id (make it a column for convenience)\n",
    "camels_gdf = camels_gdf.copy()\n",
    "camels_gdf[\"gauge_id\"] = camels_gdf.index.astype(str)\n",
    "\n",
    "# Step 3: Check if current gauge is in CAMELS\n",
    "if gauge_id in camels_gdf.index:\n",
    "    print(f\"✓ Gauge {gauge_id} found in CAMELS dataset.\")\n",
    "\n",
    "    # Step 4: Get the CAMELS attribute row\n",
    "    camels_row = camels_gdf.loc[gauge_id]\n",
    "\n",
    "    # Step 5: Get our extracted attributes\n",
    "    if \"final_df\" in locals():\n",
    "        our_row = final_df[final_df[\"gauge_id\"] == gauge_id].iloc[0]\n",
    "    else:\n",
    "        raise ValueError(\"`final_df` not found in namespace. Ensure your data is loaded.\")\n",
    "\n",
    "    # Step 6: Select common numeric attributes\n",
    "    common_cols = list(set(camels_row.index).intersection(set(our_row.index)))\n",
    "    numeric_cols = [\n",
    "        col for col in common_cols\n",
    "        if pd.api.types.is_number(camels_row[col]) and pd.api.types.is_number(our_row[col])\n",
    "    ]\n",
    "\n",
    "    # Align series to the same order\n",
    "    camels_vals = camels_row[numeric_cols].astype(float).reindex(numeric_cols)\n",
    "    our_vals = our_row[numeric_cols].astype(float).reindex(numeric_cols)\n",
    "\n",
    "    # Step 7: Compute absolute and percentage differences\n",
    "    epsilon = 1e-6  # avoid divide-by-zero\n",
    "    diff_abs = (our_vals.values - camels_vals.values).astype(float)\n",
    "    diff_pct = ((our_vals.values - camels_vals.values) / (camels_vals.values + epsilon)) * 100\n",
    "\n",
    "    # Step 8: Create stacked plots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Absolute difference plot\n",
    "    pd.Series(diff_abs, index=numeric_cols).plot(\n",
    "        kind=\"bar\", ax=axes[0], color=\"coral\", edgecolor=\"black\"\n",
    "    )\n",
    "    axes[0].set_ylabel(\"Absolute Difference\")\n",
    "    axes[0].set_title(f\"Absolute Differences vs CAMELS (Gauge {gauge_id})\")\n",
    "    axes[0].grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    axes[0].axhline(0, color=\"black\", linewidth=1)\n",
    "    axes[0].tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "    # Percent difference plot\n",
    "    pd.Series(diff_pct, index=numeric_cols).plot(\n",
    "        kind=\"bar\", ax=axes[1], color=\"skyblue\", edgecolor=\"black\"\n",
    "    )\n",
    "    axes[1].set_ylabel(\"Difference (%) relative to CAMELS\")\n",
    "    axes[1].set_title(f\"Percentage Differences vs CAMELS (Gauge {gauge_id})\")\n",
    "    axes[1].grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    axes[1].axhline(0, color=\"black\", linewidth=1)\n",
    "    axes[1].tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Print values side by side\n",
    "    comparison_df = pd.DataFrame({\n",
    "        \"CAMELS\": camels_vals.values,\n",
    "        \"Extracted\": our_vals.values,\n",
    "        \"AbsDiff\": diff_abs,\n",
    "        \"PctDiff\": diff_pct\n",
    "    }, index=numeric_cols).sort_values(\"AbsDiff\", ascending=False)\n",
    "\n",
    "    display(comparison_df)\n",
    "\n",
    "else:\n",
    "    print(f\"✗ Gauge {gauge_id} not found in CAMELS dataset. Skipping comparison.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d52b2",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Python Package: camels-attrs (Automating Attribute Extraction)</span>\n",
    "<p style='text-align: justify;'>\n",
    "To streamline the extraction of CAMELS attributes for any USGS gauge, we have developed a dedicated Python library: <b>camels-attrs</b>. \n",
    "This package encapsulates the methodology presented in this tutorial into a simple, easy-to-use interface, handling data retrieval, processing, and attribute computation automatically.\n",
    "</p>\n",
    "\n",
    "#### <span style=\"color:green\">Installation</span>\n",
    "You can install the package directly via pip:\n",
    "```bash\n",
    "pip install camels-attrs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be798e2d",
   "metadata": {},
   "source": [
    "<a id=\"13-extending-to-multiple-basins\"></a>\n",
    "## <span style=\"color:green\">13. Extending to Multiple Basins</span>\n",
    "\n",
    "### <span style=\"color:green\">Learning Objectives</span>\n",
    "- Scale the workflow to process multiple basins  \n",
    "- Implement batch processing and error handling  \n",
    "- Manage computational resources efficiently  \n",
    "\n",
    "### <span style=\"color:green\">Batch Processing Framework</span>\n",
    "<p style='text-align: justify;'>\n",
    "To move from a single-basin prototype to a <b>large-sample hydrology workflow</b>, we must extend the pipeline to handle multiple basins. \n",
    "This is achieved through <b>batch processing</b>, where each gauge is processed in sequence (or parallelized) with robust error handling and logging. \n",
    "Successful results are aggregated into a <b>combined CAMELS-like dataset</b>, while failures are tracked for troubleshooting. \n",
    "The framework also generates <b>summary statistics</b> and <b>correlation diagnostics</b> to provide insights into dataset completeness and attribute consistency across basins.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Key features of the batch processing framework include:  \n",
    "- Automated watershed delineation and attribute extraction for each gauge  \n",
    "- Integration of results into both <b>individual basin files</b> and a <b>national-scale combined dataset</b>  \n",
    "- Export of summary statistics (attribute ranges, correlations, completeness)  \n",
    "- Scalable design, with potential for <b>parallel or distributed execution</b> on HPC or cloud environments  \n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This framework ensures reproducibility, scalability, and efficiency, enabling the creation of CAMELS-like datasets for hundreds or thousands of basins simultaneously.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9964b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_basins(gauge_list, output_dir=\"outputs/batch_processing\"):\n",
    "    \"\"\"\n",
    "    Process multiple basins using the CAMELS-like extraction workflow\n",
    "    \n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/individual_basins\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/combined_datasets\", exist_ok=True)\n",
    "    os.makedirs(f\"{output_dir}/processing_logs\", exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting batch processing for {len(gauge_list)} basins...\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Initialize tracking\n",
    "    results_summary = {\n",
    "        'total_basins': len(gauge_list),\n",
    "        'successful': [],\n",
    "        'failed': [],\n",
    "        'processing_start': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'processing_end': None,\n",
    "        'individual_results': {}\n",
    "    }\n",
    "    \n",
    "    all_basin_data = []\n",
    "    \n",
    "    # ------------------------------\n",
    "    # Safe serialization helper\n",
    "    # ------------------------------\n",
    "    def safe_serialize(obj, gauge_id=None):\n",
    "        \"\"\"Convert non-serializable objects for JSON export.\"\"\"\n",
    "        import pandas as pd\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            # Save DataFrame to CSV instead of bloating JSON\n",
    "            if gauge_id:\n",
    "                csv_path = f\"{output_dir}/individual_basins/{gauge_id}_data.csv\"\n",
    "                obj.to_csv(csv_path, index=False)\n",
    "                return {\"dataframe_saved\": csv_path, \"rows\": len(obj), \"cols\": list(obj.columns)}\n",
    "            return obj.to_dict(orient=\"records\")\n",
    "        elif isinstance(obj, pd.Series):\n",
    "            return obj.to_dict()\n",
    "        elif isinstance(obj, (set,)):\n",
    "            return list(obj)\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Process each basin\n",
    "    for i, gauge_id in enumerate(gauge_list):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing basin {i+1}/{len(gauge_list)}: {gauge_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Process single basin\n",
    "            basin_result = process_single_basin_batch(gauge_id, output_dir)\n",
    "            \n",
    "            # Handle success/failure\n",
    "            if basin_result['success']:\n",
    "                results_summary['successful'].append(gauge_id)\n",
    "                all_basin_data.append(basin_result['data'])\n",
    "                print(f\"✓ Successfully processed gauge {gauge_id}\")\n",
    "            else:\n",
    "                results_summary['failed'].append(gauge_id)\n",
    "                print(f\"✗ Failed to process gauge {gauge_id}: {basin_result['error']}\")\n",
    "            \n",
    "            # Store sanitized results\n",
    "            results_summary['individual_results'][gauge_id] = {\n",
    "                k: safe_serialize(v, gauge_id) for k, v in basin_result.items()\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Unexpected error processing gauge {gauge_id}: {str(e)}\")\n",
    "            results_summary['failed'].append(gauge_id)\n",
    "            results_summary['individual_results'][gauge_id] = {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'data': None\n",
    "            }\n",
    "    \n",
    "    # Create combined dataset if possible\n",
    "    if all_basin_data:\n",
    "        print(f\"\\nCreating combined dataset for {len(all_basin_data)} successful basins...\")\n",
    "        combined_df = pd.concat(all_basin_data, ignore_index=True)\n",
    "        \n",
    "        # Export combined dataset\n",
    "        combined_path = f\"{output_dir}/combined_datasets/camels_like_dataset.csv\"\n",
    "        combined_df.to_csv(combined_path, index=False)\n",
    "        print(f\"✓ Combined dataset saved: {combined_path}\")\n",
    "        \n",
    "        # Create summary statistics\n",
    "        create_batch_summary_stats(combined_df, output_dir)\n",
    "        \n",
    "        # Add reference only (not whole DataFrame!)\n",
    "        results_summary['combined_dataset'] = combined_path\n",
    "    \n",
    "    # Finalize results\n",
    "    results_summary['processing_end'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    results_summary['success_rate'] = len(results_summary['successful']) / len(gauge_list)\n",
    "    \n",
    "    # Save processing summary (JSON safe now)\n",
    "    summary_path = f\"{output_dir}/processing_summary.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total basins processed: {len(gauge_list)}\")\n",
    "    print(f\"Successful: {len(results_summary['successful'])} ({results_summary['success_rate']:.1%})\")\n",
    "    print(f\"Failed: {len(results_summary['failed'])}\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "def process_single_basin_batch(gauge_id, output_dir):\n",
    "    \"\"\"\n",
    "    Process a single basin in batch mode with error handling\n",
    "    \n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'gauge_id': gauge_id,\n",
    "        'success': False,\n",
    "        'error': None,\n",
    "        'data': None,\n",
    "        'processing_time': None,\n",
    "        'attributes_extracted': 0\n",
    "    }\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # 1. Watershed delineation\n",
    "        watershed_gdf, watershed_geom, metadata, area_km2 = delineate_watershed(gauge_id, save_shapefile=False)\n",
    "        if watershed_gdf is None:\n",
    "            raise Exception(\"Failed to delineate watershed\")\n",
    "        \n",
    "        # 2. Extract all attributes with error handling\n",
    "        print(\"  Extracting topographic attributes...\")\n",
    "        topo_attrs, _, _,_ = extract_topographic_attributes(watershed_geom)\n",
    "        if topo_attrs is None:\n",
    "            topo_attrs = {'elev_mean': np.nan, 'slope_mean': np.nan, 'area_geospa_fabric': np.nan}\n",
    "        \n",
    "        print(\"  Extracting climate attributes...\")\n",
    "        climate_ds = fetch_climate_data(watershed_geom, start_date=\"2010-01-01\", end_date=\"2020-12-31\")\n",
    "        if climate_ds is not None:\n",
    "            climate_attrs = compute_climate_indices(climate_ds)\n",
    "        else:\n",
    "            climate_attrs = {'p_mean': np.nan, 'pet_mean': np.nan, 'aridity': np.nan}\n",
    "        \n",
    "        print(\"  Extracting soil attributes...\")\n",
    "        soil_attrs = extract_soil_attributes(watershed_geom)\n",
    "        \n",
    "        print(\"  Extracting vegetation attributes...\")\n",
    "        veg_attrs = extract_vegetation_attributes(watershed_geom,gauge_id)\n",
    "        \n",
    "        print(\"  Extracting geological attributes...\")\n",
    "        geol_attrs = extract_geological_attributes(watershed_gdf)\n",
    "        \n",
    "        print(\"  Extracting hydrological signatures...\")\n",
    "        hydro_attrs = extract_hydrological_signatures(gauge_id, watershed_geom, \n",
    "                                                     start_date=\"2010-01-01\", end_date=\"2020-12-31\")\n",
    "        \n",
    "        # 3. Integrate attributes\n",
    "        \n",
    "        basin_df, _ = integrate_all_attributes(\n",
    "        gauge_id,\n",
    "        topo_attrs,\n",
    "        climate_attrs[0],   # take dict out of tuple\n",
    "        soil_attrs[0],\n",
    "        veg_attrs,\n",
    "        geol_attrs[0],\n",
    "        hydro_attrs,\n",
    "        metadata=metadata\n",
    "        )\n",
    "\n",
    "        \n",
    "        if basin_df is None:\n",
    "            raise Exception(\"Failed to integrate attributes\")\n",
    "        \n",
    "        # 4. Save individual basin results\n",
    "        individual_dir = f\"{output_dir}/individual_basins/{gauge_id}\"\n",
    "        os.makedirs(individual_dir, exist_ok=True)\n",
    "        \n",
    "        basin_path = f\"{individual_dir}/attributes_{gauge_id}.csv\"\n",
    "        basin_df.to_csv(basin_path, index=False)\n",
    "        \n",
    "        # 5. Update result\n",
    "        result['success'] = True\n",
    "        result['data'] = basin_df\n",
    "        result['attributes_extracted'] = len(basin_df.columns)\n",
    "        result['processing_time'] = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "    except Exception as e:\n",
    "        result['error'] = str(e)\n",
    "        result['processing_time'] = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_batch_summary_stats(combined_df, output_dir):\n",
    "    \"\"\"Create summary statistics for the batch processing results\"\"\"\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats_summary = {\n",
    "        'total_basins': len(combined_df),\n",
    "        'total_attributes': len(combined_df.columns),\n",
    "        'data_completeness': (combined_df.notna().sum() / len(combined_df)).to_dict(),\n",
    "        'attribute_ranges': {},\n",
    "        'correlations': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate ranges for numerical attributes\n",
    "    numerical_cols = combined_df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numerical_cols:\n",
    "        if col in combined_df.columns:\n",
    "            stats_summary['attribute_ranges'][col] = {\n",
    "                'min': float(combined_df[col].min()),\n",
    "                'max': float(combined_df[col].max()),\n",
    "                'mean': float(combined_df[col].mean()),\n",
    "                'std': float(combined_df[col].std())\n",
    "            }\n",
    "    \n",
    "    # Save summary statistics\n",
    "    stats_path = f\"{output_dir}/combined_datasets/summary_statistics.json\"\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(stats_summary, f, indent=2)\n",
    "    \n",
    "    # Create correlation matrix for key attributes\n",
    "    key_attrs = ['elev_mean', 'slope_mean', 'p_mean', 'aridity', 'runoff_ratio', \n",
    "                'q_mean', 'baseflow_index']\n",
    "    available_attrs = [attr for attr in key_attrs if attr in combined_df.columns]\n",
    "    \n",
    "    if len(available_attrs) > 3:\n",
    "        corr_matrix = combined_df[available_attrs].corr()\n",
    "        \n",
    "        # Plot correlation matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(corr_matrix, cmap='RdBu', vmin=-1, vmax=1)\n",
    "        plt.colorbar(label='Correlation Coefficient')\n",
    "        plt.xticks(range(len(available_attrs)), available_attrs, rotation=45)\n",
    "        plt.yticks(range(len(available_attrs)), available_attrs)\n",
    "        plt.title('Correlation Matrix of Key Attributes')\n",
    "        \n",
    "        # Add correlation values\n",
    "        for i in range(len(available_attrs)):\n",
    "            for j in range(len(available_attrs)):\n",
    "                plt.text(j, i, f'{corr_matrix.iloc[i,j]:.2f}', \n",
    "                        ha='center', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/combined_datasets/correlation_matrix.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"✓ Summary statistics created for {len(combined_df)} basins\")\n",
    "\n",
    "# ================================\n",
    "# Batch Processing Entry Point\n",
    "# ================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: replace with your full list of USGS gauge IDs\n",
    "    gauge_list = [\n",
    "        \"01054200\",  # Maine\n",
    "        \"02177000\",  # South Carolina\n",
    "        \"06803530\",  # Nebraska\n",
    "        \"09480000\",  # Arizona\n",
    "        \"12447383\"   # Washington\n",
    "    ]\n",
    "\n",
    "    # Run batch processing\n",
    "    results = process_multiple_basins(gauge_list)\n",
    "\n",
    "    # Print a short summary\n",
    "    print(\"\\nBatch run finished.\")\n",
    "    print(f\"Total: {results['total_basins']}, \"\n",
    "          f\"Successful: {len(results['successful'])}, \"\n",
    "          f\"Failed: {len(results['failed'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4b90b",
   "metadata": {},
   "source": [
    "<a id=\"14-summary\"></a>\n",
    "## <span style=\"color:green\">14. Summary</span>\n",
    "\n",
    "### Key Achievements\n",
    "In this tutorial, we:\n",
    "1. Replicated **CAMELS methodology** for any USGS basin  \n",
    "2. Integrated **multiple open datasets** (topography, climate, soils, vegetation, geology, hydrology)  \n",
    "3. Computed a **comprehensive attribute set** for catchment analysis  \n",
    "4. Implemented **validation**  \n",
    "5. Demonstrated **scalability** to large-sample hydrology  \n",
    "\n",
    "### References\n",
    "- Newman et al. (2015), Addor et al. (2017) – Original CAMELS dataset  \n",
    "- USGS 3DEP, GridMET, gNATSGO, MODIS, GLiM, GLHYMPS – Data sources  \n",
    "- HyRiver ecosystem, Planetary Computer SDK, GeoPandas – Core tools  \n",
    "\n",
    "---\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This framework makes CAMELS-like datasets reproducible, scalable, and aligned with FAIR principles. It enables both single-basin studies and national-scale hydrology, supporting open science and next-generation hydrological modeling.\n",
    "</p>\n",
    "\n",
    "*For questions or issues, contact: mgalib@purdue.edu or vmerwade@purdue.edu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52501ad6",
   "metadata": {},
   "source": [
    "# CAMELS Attributes Table\n",
    "\n",
    "This document lists the attribute groups and their descriptions, units, data sources, and references for the original CAMELS dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## camels_name – Name\n",
    "| Attribute  | Description | Unit | Data source | References |\n",
    "|------------|-------------|------|-------------|------------|\n",
    "| gauge_id   | catchment identifier (8-digit USGS hydrologic unit code) | - | N15 – USGS data | |\n",
    "| huc_02     | region (2-digit USGS hydrologic unit code) | - | N15 – USGS data | |\n",
    "| gauge_name | gauge name, followed by the state | - | N15 – USGS data | |\n",
    "\n",
    "---\n",
    "\n",
    "## camels_topo – Topography and location\n",
    "| Attribute | Description | Unit | Data source | References |\n",
    "|-----------|-------------|------|-------------|------------|\n",
    "| gauge_lat | gauge latitude | ° north | N15 – USGS data | |\n",
    "| gauge_lon | gauge longitude | ° east | N15 – USGS data | |\n",
    "| elev_mean | catchment mean elevation | m above sea level | N15 – USGS data | |\n",
    "| slope_mean | catchment mean slope | m/km | N15 – USGS data | |\n",
    "| area_gages2 | catchment area (GAGESII estimate) | km² | N15 – USGS data | Falcone (2011) |\n",
    "| area_geospa_fabric | catchment area (Geospatial Fabric estimate) | km² | N15 – Geospatial Fabric | Viger (2014); Viger & Bock (2014) |\n",
    "\n",
    "---\n",
    "\n",
    "## camels_clim – Climate indices (*1989/10/01–2009/09/30*)\n",
    "| Attribute | Description | Unit | Data source | References |\n",
    "|-----------|-------------|------|-------------|------------|\n",
    "| p_mean | mean daily precipitation | mm/day | N15 – Daymet | |\n",
    "| pet_mean | mean daily PET (Priestley-Taylor calibrated) | mm/day | N15 – Daymet | |\n",
    "| aridity | aridity index (PET/P) | - | N15 – Daymet | |\n",
    "| p_seasonality | precipitation seasonality & timing (sine curves, + = summer, – = winter) | - | N15 – Daymet | Woods et al. (2009), Eq. 14 |\n",
    "| frac_snow_daily | fraction of precipitation falling as snow (T < 0°C) | - | N15 – Daymet | |\n",
    "| high_prec_freq | frequency of high-precipitation days (≥ 5× mean daily P) | days/yr | N15 – Daymet | |\n",
    "| high_prec_dur | average duration of high-precipitation events | days | N15 – Daymet | |\n",
    "| high_prec_timing | season of most high-precipitation days | season | N15 – Daymet | |\n",
    "| low_prec_freq | frequency of dry days (< 1 mm/day) | days/yr | N15 – Daymet | |\n",
    "| low_prec_dur | average duration of dry periods | days | N15 – Daymet | |\n",
    "| low_prec_timing | season of most dry days | season | N15 – Daymet | |\n",
    "\n",
    "---\n",
    "\n",
    "## camels_hydro – Hydrological signatures (*1989/10/01–2009/09/30*)\n",
    "| Attribute | Description | Unit | Data source | References |\n",
    "|-----------|-------------|------|-------------|------------|\n",
    "| q_mean | mean daily discharge | mm/day | N15 – USGS data | |\n",
    "| runoff_ratio | runoff ratio (Q/P) | - | N15 – USGS data | Sawicz et al. (2011), Eq. 2 |\n",
    "| stream_elas | streamflow–precipitation elasticity | - | N15 – USGS data | Sankarasubramanian et al. (2001), Eq. 7 |\n",
    "| slope_fdc | slope of flow duration curve (log Q33–Q66) | - | N15 – USGS data | Sawicz et al. (2011), Eq. 3 |\n",
    "| baseflow_index | baseflow index (BF/Q, digital filter) | - | N15 – USGS data | Ladson et al. (2013) |\n",
    "| hfd_mean | mean half-flow date (day of year) | DOY | N15 – USGS data | Court (1962) |\n",
    "| Q5 | 5% flow quantile (low flow) | mm/day | N15 – USGS data | |\n",
    "| Q95 | 95% flow quantile (high flow) | mm/day | N15 – USGS data | |\n",
    "| high_q_freq | frequency of high-flow days (> 9× median flow) | days/yr | N15 – USGS data | Clausen & Biggs (2000); Westerberg & McMillan (2015) |\n",
    "| high_q_dur | duration of high-flow events | days | N15 – USGS data | Clausen & Biggs (2000); Westerberg & McMillan (2015) |\n",
    "| low_q_freq | frequency of low-flow days (< 0.2× mean flow) | days/yr | N15 – USGS data | Olden & Poff (2003); Westerberg & McMillan (2015) |\n",
    "| low_q_dur | duration of low-flow events | days | N15 – USGS data | Olden & Poff (2003); Westerberg & McMillan (2015) |\n",
    "| zero_q_freq | frequency of zero-flow days | % | N15 – USGS data | |\n",
    "\n",
    "---\n",
    "\n",
    "## camels_vege – Land cover characteristics (*2002–2014*)\n",
    "| Attribute | Description | Unit | Data source | References |\n",
    "|-----------|-------------|------|-------------|------------|\n",
    "| forest_frac | forest fraction | - | N15 – USGS data | |\n",
    "| lai_max | maximum monthly mean LAI | - | MODIS | |\n",
    "| lai_diff | seasonal LAI difference (max–min) | - | MODIS | |\n",
    "| gvf_max | maximum monthly mean green vegetation fraction | - | MODIS | |\n",
    "| gvf_diff | seasonal GVF difference (max–min) | - | MODIS | |\n",
    "| dom_land_cover | dominant land cover type (20-class IGBP-MODIS) | - | MODIS | |\n",
    "| dom_land_cover_frac | fraction of area under dominant land cover | - | MODIS | |\n",
    "| root_depth_XX | root depth (percentiles 50%, 99%) | m | MODIS | Zeng (2001), Eq. 2, Table 2 |\n",
    "\n",
    "---\n",
    "\n",
    "## camels_soil – Soil characteristics (*top 1.5 m*)\n",
    "| Attribute | Description | Unit | Data source | References |\n",
    "|-----------|-------------|------|-------------|------------|\n",
    "| soil_depth_pelletier | depth to bedrock (max 50 m) | m | Pelletier et al. (2016) | |\n",
    "| soil_depth_statgso | soil depth (max 1.5 m) | m | STATSGO | Miller & White (1998) |\n",
    "| soil_porosity | volumetric porosity (saturated water content) | - | STATSGO | Cosby et al. (1984); Lawrence & Slater (2008) |\n",
    "| soil_conductivity | saturated hydraulic conductivity | cm/hr | STATSGO | Cosby et al. (1984); Lawrence & Slater (2008) |\n",
    "| max_water_content | maximum water content (porosity × depth) | m | STATSGO | |\n",
    "| sand_frac | sand fraction (< 2 mm) | % | STATSGO | |\n",
    "| silt_frac | silt fraction (< 2 mm) | % | STATSGO | |\n",
    "| clay_frac | clay fraction (< 2 mm) | % | STATSGO | |\n",
    "| water_frac | fraction marked as water (class 14) | % | STATSGO | |\n",
    "| organic_frac | fraction marked as organic material (class 13) | % | STATSGO | |\n",
    "| other_frac | fraction marked as “other” (class 16) | % | STATSGO | |\n",
    "\n",
    "---\n",
    "\n",
    "## camels_geol – Geological characteristics\n",
    "| Attribute | Description | Unit | Data source | References |\n",
    "|-----------|-------------|------|-------------|------------|\n",
    "| geol_class_1st | most common geologic class | - | GLiM | Hartmann & Moosdorf (2012) |\n",
    "| geol_class_1st_frac | fraction of catchment in most common class | - | GLiM | Hartmann & Moosdorf (2012) |\n",
    "| geol_class_2nd | 2nd most common geologic class | - | GLiM | Hartmann & Moosdorf (2012) |\n",
    "| geol_class_2nd_frac | fraction of catchment in 2nd class | - | GLiM | Hartmann & Moosdorf (2012) |\n",
    "| carb_rocks_frac | fraction of carbonate sedimentary rocks | - | GLiM | Hartmann & Moosdorf (2012) |\n",
    "| geol_porosity | subsurface porosity | - | GLHYMPS | Gleeson et al. (2014) |\n",
    "| geol_permeability | subsurface permeability (log10) | m² | GLHYMPS | Gleeson et al. (2014) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb7e083",
   "metadata": {},
   "source": [
    "| Attribute  | Description | Unit | Data source | References |\n",
    "|------------|-------------|------|-------------|------------|\n",
    "| gauge_id   | catchment identifier | - | USGS | |\n",
    "| huc_02     | region code | - | USGS | |\n",
    "| gauge_name | gauge name | - | USGS | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30d9cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct-aiml",
   "language": "python",
   "name": "ct-aiml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
